<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[git的基本命令]]></title>
    <url>%2F2017%2F10%2F26%2Fgit%E7%9A%84%E5%9F%BA%E6%9C%AC%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[本地目录变成git仓库1git init 把工作区的修改提交到Git仓库的暂存区1git add file 把暂存区的所有内容提交到当前分支1git commit -m &quot;summary&quot; 查看当前分支修改状态1git status 远程把本地仓库添加远程库(GitHub)1git remote add origin git@github.com:yourname/repository.git 把本地库master的所有内容推送到远程库1git push origin master 从远程库克隆出一个本地库1git clone git@github.com:yourname/repository.git 从远程获取最新版本到本地1git fetch origin master:temp 比较本地的仓库和远程参考的区别1git diff temp 合并temp分支到master分支1git merge temp 删除此分支1git branch -d temp 移除远程库1git remote rm origin]]></content>
  </entry>
  <entry>
    <title><![CDATA[经典卷积神经网络（AlexNet）]]></title>
    <url>%2F2017%2F10%2F17%2F%E7%BB%8F%E5%85%B8%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88AlexNet%EF%BC%89%2F</url>
    <content type="text"><![CDATA[写在前面不同的卷积神经网络的区别，其实就是网络的结构不同。本文介绍最经典的CNN——AlexNet。 AlexNet网络结构 AlexNet总共包括8层，其中前5层卷积层，后面3层是全连接层（图中尺寸有问题）。| 操作层 | 输入尺寸 |参数 | 输出尺寸 || :————- | :————- |:————- |:————- || 输入层 | [227x227x3]| | [227x227x3] || 卷积层1 | [227x227x3]| 核[11,11,3,96] strides=[1,4,4,1]|[55x55x96]|| 池化层1 | [55x55x96]| 核[1,3,3,1] strides=[1,2,2,1]|[27x27x96]|| 卷积层2 | [27x27x96]| 核[5,5,96,256] strides=[1,1,1,1] pad=2]|[27x27x256]|| 池化层2 | [27x27x256]| 核[1,3,3,1] strides=[1,2,2,1]|[13x13x256]|| 卷积层3 | [13x13x256]| 核[3,3,256,384] strides=[1,1,1,1] pad=1|[13x13x384]|| 卷积层4 | [13x13x384]| 核[3,3,384,384] strides=[1,1,1,1] pad=1|[13x13x384]|| 卷积层5 | [13x13x384]| 核[3,3,384,256] strides=[1,1,1,1] pad=1|[13x13x256]|| 池化层3 | [13x13x256]| 核[1,3,3,1] strides=[1,2,2,1] pad=1|[6x6x256]|| 全连接层1 | [6x6x256] | [4096] |[4096]|| 全连接层2 | [4096] | [4096] | [4096]|| softmax | [4096] | [1000] |[1000]| 特点 针对网络架构: 使用ReLU作为激活函数，收敛速度快，并验证其效果在较深的网络要优于Sigmoid。 使用LRN层，对局部神经元的活动创建竞争机制，使得其中响应比较大的值变的相对更大，并抑制其他反馈较小的神经元，增强了模型的泛化能力。 使用重叠的最大池化，让步长比池化核的尺寸小，池化层的输出之间会有重叠，提升了特征的丰富性。 针对过拟合现象: 数据增强，对原始图像随机截取输入图片尺寸大小(以及对图像作水平翻转操作)，使用数据增强后大大减轻过拟合，提升模型的泛化能力。 使用Dropout随机忽略一部分神经元，避免模型过拟合。 针对训练速度: 使用GPU。 AlexNet 应用 MNIST1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192# coding:utf8from tensorflow.examples.tutorials.mnist import input_dataimport tensorflow as tfdef initialize_weight(shape, stddev, name): initial = tf.truncated_normal(shape, dtype = tf.float32, stddev = stddev) return tf.Variable(initial, name = name)def initialize_bias(shape): initial = tf.random_normal(shape) return tf.Variable(initial)def conv2d(x, w, b): return tf.nn.relu((tf.nn.conv2d(x, w, strides = [1, 1, 1, 1], padding = &apos;SAME&apos;) + b))def max_pool(x, f): return tf.nn.max_pool(x, ksize = [1, f, f, 1], strides = [1, 1, 1, 1], padding = &apos;SAME&apos;)# Create AlexNet Modelx = tf.placeholder(tf.float32, [None, 784])x_image = tf.reshape(x, shape = [-1, 28, 28, 1])y = tf.placeholder(tf.float32, [None, 10])# layer1: conv2dw1_conv = initialize_weight([3, 3, 1, 64], 0.1, &apos;w1&apos;)b1_conv = initialize_bias([64])h1_conv = conv2d(x_image, w1_conv, b1_conv)#28x28x1&gt;&gt;28x28x64h1_pool = max_pool(h1_conv, 2)#28x28x64&gt;&gt;14x14x64# layer2: conv2dw2_conv = initialize_weight([3, 3, 64, 64], 0.1, &apos;w2&apos;)b2_conv = initialize_bias([64])h2_conv = conv2d(h1_pool, w2_conv, b2_conv)#14x14x64&gt;&gt;14x14x64h2_pool = max_pool(h2_conv, 2)#14x14x64&gt;&gt;7x7x64# layer3: conv2dw3_conv = initialize_weight([3, 3, 64, 128], 0.1, &apos;w3&apos;)b3_conv = initialize_bias([128])h3_conv = conv2d(h2_pool, w3_conv, b3_conv)#7x7x64&gt;&gt;7x7x128# layer4: conv2dw4_conv = initialize_weight([3, 3, 128, 128], 0.1, &apos;w4&apos;)b4_conv = initialize_bias([128])h4_conv = conv2d(h3_conv, w4_conv, b4_conv)#7x7x128&gt;&gt;7x7x128# layer5: conv2dw5_conv = initialize_weight([3, 3, 128, 256], 0.1, &apos;w5&apos;)b5_conv = initialize_bias([256])h5_conv = conv2d(h4_conv, w5_conv, b5_conv)#7x7x128&gt;&gt;7x7x256h5_pool = max_pool(h5_conv, 2)#shape = h5_pool.get_shape()h5_pool_flat = tf.reshape(h5_pool, [-1, shape[1].value*shape[2].value*shape[3].value])# layer6: full connectionw6_fc = initialize_weight([256*28*28, 1024], 0.01, &apos;w6&apos;)b6_fc = initialize_bias([1024])h6_fc = tf.nn.relu(tf.matmul(h5_pool_flat, w6_fc) + b6_fc)keep_prob = tf.placeholder(&apos;float&apos;)h6_drop = tf.nn.dropout(h6_fc, keep_prob = keep_prob)# layer7: full connectionw7_fc = initialize_weight([1024, 1024], 0.01, &apos;w7&apos;)b7_fc = initialize_bias([1024])h7_fc = tf.nn.relu(tf.matmul(h6_drop, w7_fc) + b7_fc)h7_drop = tf.nn.dropout(h7_fc, keep_prob = keep_prob)# layer8: softmaxw8_sf = initialize_weight([1024, 10], 0.01, &apos;w8&apos;)b8_sf = initialize_bias([10])y_conv = tf.nn.softmax(tf.matmul(h7_drop, w8_sf) + b8_sf)cross_entropy = -tf.reduce_sum(y * tf.log(y_conv))train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y, 1))accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))# Train AlexNet Modelmnist = input_data.read_data_sets(&apos;MNIST_data/&apos;, one_hot = True)sess = tf.Session()sess.run(tf.global_variables_initializer())for i in range(1500): batch = mnist.train.next_batch(64) if i % 10 == 0: train_accuracy = sess.run(accuracy, feed_dict = &#123;x: batch[0], y: batch[1], keep_prob: 0.9&#125;) print(&apos;step %d, training accuracy %g&apos; % (i, train_accuracy)) sess.run(train_step, feed_dict = &#123;x: batch[0], y: batch[1], keep_prob: 0.9&#125;)test_accuracy = sess.run(accuracy, feed_dict = &#123;x: mnist.test.images, y: mnist.test.labels, keep_prob: 0.9&#125;)print(&apos;test accuracy %g&apos; % test_accuracy)sess.close() 参考：http://blog.csdn.net/u011974639/article/details/76146822#alexnet的特点]]></content>
  </entry>
  <entry>
    <title><![CDATA[卷积神经网络(Convolutional Neural Network）]]></title>
    <url>%2F2017%2F10%2F14%2F%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C(Convolutional%20Neural%20Network%EF%BC%89%2F</url>
    <content type="text"><![CDATA[卷积神经网络卷积神经网络 (convolutional neural network, CNN)，是一种专门用来处理具有网格结构数据的神经网络。例如时间序列数据(时间轴上的一维网格)和图像数据(二维的像素网格)。 CNN作为一个深度学习架构被提出的最初诉求，是降低对图像数据预处理的要求，以及避免复杂的特征工程。CNN可以直接使用图像作为输入，减少了许多特征提取的手续。CNN的最大特点在于卷积的权值共享结构，大幅度减少神经网络的参数，同时防止了过拟合。 卷积神经网络结构一般来说，卷积神经网络主要由以下结构组成: 输入层输入层是整个神经网络的输入，一般为图片的像素矩阵(一般为三维矩阵，即像素x像素x通道) 卷积层卷积层的作用是从输入层中抽象出更高的特征。一般流程如下： 图像通过多个不同的卷积核滤波，并添加偏置(bias)，提取局部特征，每一个卷积核会映射出一个新的2D图像。 将前面卷积核的滤波输出结果，进行非线性的激活函数处理。 对激活函数的结构再进行池化操作(降采样），目前一般使用最大池化，保留最大特征，提示模型的畸变容忍能力。 卷积图像处理中的卷积计算，依赖于卷积核Kernel，而卷积核就是一个滤波器，卷积核与输入信号做加权叠加得到输出。 卷积参数（以上图为例） 输入尺寸4x4 卷积核尺寸3x3 输出尺寸2x2 移动步长（stride）1 可以发现，只要卷积核的尺寸不是1x1,那么输出尺寸必定会小于输入尺寸大小。而在实际图片处理过程中，很多时候需要保持图像的大小不变。为了保持输入和输出尺寸一致，同时也有卷积的效果，我们对输入图片的外圈做填充操作。 因此，引入新的参数padding，即在输入的外圈补零的圈数(数字零对卷积贡献为零,不会产生额外的偏差)。 这里给出输入尺寸、输出尺寸、滤波器尺寸、stride和padding的关系: 输入图片的尺寸大小W1 x H1 卷积核(又称滤波器，以下都称滤波器)的大小F x F stride：S padding:P 输出图片的尺寸大小W2 x H2 $W2 = (W1-F+2P)/S + 1$ $H2 = (H1-F+2P)/S + 1$ 权值共享每一个卷积层中使用的过滤器参数是相同的，这就是卷积核的权值共享。 从直观上理解，共享卷积核可以使得图像上的内容不受位置的影响，这提高了模型对平移的容忍性，这大大的提高了模型提取特征的能力。 从网络结构上来说，共享每一个卷积层的卷积核，可以大大减少网络的参数，这不仅可以降低计算的复杂度，而且还能减少因为连接过多导致的严重过拟合，从而提高模型的泛化能力。 权值共享与传统的全连接的区别如下图： 多卷积核在每一个卷积层中，会使用多个卷积核运算。这是因为每一个卷积核滤波得到的图像就是一类特征的映射，使用多个卷积核，就能提供多个方向上的图像特征，可以从图像中抽象出有效而丰富的高阶特征。 池化层池化层可以非常有效地缩小图片的尺寸，减少最后全连接层的参数，保留最显著的特征，提升模型的畸变容忍能力，防止过拟合。 池化层前向传播过程类似于卷积层，是通过移动一个类似滤波器的结构完成的。与卷积层不同的是，池化层的滤波器计算不是加权求和，而且求区域内的极大值或者平均值。 最大池化层(max pooling)，最常使用，计算图像区域内最大值，提取纹理效果较好。 平均池化层(mean pooling)，计算图像区域的平均值，保留背景更好。 随机池化层(stochastic pooling)，介于两者之间，通过对像素点按照数值大小赋予概率，再按照概率进行亚采样。 全连接层图像中被抽象成了信息含量更高的特征在经过神经网络完成后续分类等任务。 输出层一般是使用softmax输出概率值或者分类结果。 TensorFlow部署CNN卷积通道混合处理的任意滤波器1tf.nn.conv2d(x, W, strides, padding) strides = [batch, width, height, channels] batch = 1表示样本一个一个遍历 width height = 1表示移动的步长 channels = 1表示通道一个一个滑动 padding = ‘SAME’,’VALID’ ‘SAME’ 输入输出尺寸相同 ‘VALID’ (w-f)/k + 1 池化1tf.nn.max_pool(x, ksize, strides, padding) ksize = [1, height, width, 1] batch = 1表示不在样本上做池化 width height表示池化窗口大小 channels = 1表示不在通道上做池化 激活函数12345tf.nn.relu(x)tf.nn.dropout(x, keep_prob)tf.nn.bias_add(x, b)tf.sigmoid(x)tf.tanh(x) CNN 应用 MNIST12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970from tensorflow.examples.tutorials.mnist import input_dataimport tensorflow as tfdef initialize_weight(shape): initial = tf.truncated_normal(shape, stddev = 0.1) return tf.Variable(initial)def initialize_bias(shape): initial = tf.constant(0.1, shape = shape) return tf.Variable(initial)def conv2d(x, W): return tf.nn.conv2d(x, W, strides = [1, 1, 1, 1], padding = &apos;SAME&apos;)def max_pool_2x2(x): return tf.nn.max_pool(x, ksize = [1, 2, 2, 1], strides = [1, 2, 2, 1], padding = &apos;SAME&apos;)#create CNN Modelx = tf.placeholder(&apos;float&apos;, [None, 784])y = tf.placeholder(&apos;float&apos;, [None, 10])x_image = tf.reshape(x, [-1,28,28,1])#28x28x1# layer1:conv2dW1_conv = initialize_weight([5, 5, 1, 32])b1_conv = initialize_bias([32])h1_conv = tf.nn.relu(conv2d(x_image, W1_conv) + b1_conv)#28x28x1&gt;&gt;28x28x32h1_pool = max_pool_2x2(h1_conv)#28x28x32&gt;&gt;14x14x32# layer2:conv2dW2_conv = initialize_weight([5, 5, 32, 64])b2_conv = initialize_bias([64])h2_conv = tf.nn.relu(conv2d(h1_pool, W2_conv) + b2_conv)#14x14x32&gt;&gt;14x14x64h2_pool = max_pool_2x2(h2_conv)#14x14x64&gt;&gt;7x7x64h2_pool_flat = tf.reshape(h2_pool, [-1, 7*7*64])#7x7x64&gt;&gt;7*7*64# layer3:full connectionW3_fc = initialize_weight([7*7*64, 1024])b3_fc = initialize_bias([1024])h3_fc = tf.nn.relu(tf.matmul(h2_pool_flat, W3_fc) + b3_fc)#7*7*64&gt;&gt;1024# drop outkeep_prob = tf.placeholder(&apos;float&apos;)h3_fc_drop = tf.nn.dropout(h3_fc, keep_prob)# layer4: full connectionW4_fc = initialize_weight([1024, 10])b4_fc = initialize_bias([10])y_conv = tf.nn.softmax(tf.matmul(h3_fc_drop, W4_fc) + b4_fc)cross_entropy = -tf.reduce_sum(y*tf.log(y_conv))train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y,1))accuracy = tf.reduce_mean(tf.cast(correct_prediction, &apos;float&apos;))# train modelmnist = input_data.read_data_sets(&apos;MNIST_data/&apos;, one_hot = True)sess = tf.Session()sess.run(tf.global_variables_initializer())for i in range(20000): batch = mnist.train.next_batch(50) if i % 100 == 0: train_accuracy = sess.run(accuracy, feed_dict = &#123;x: batch[0], y: batch[1], keep_prob: 1.0&#125;) print(&apos;step %d, training accuracy %g&apos; % (i, train_accuracy)) sess.run(train_step, feed_dict = &#123;x: batch[0], y: batch[1], keep_prob: 0.5&#125;)test_accuracy = sess.run(accuracy, feed_dict = &#123;x: mnist.test.images, y: mnist.test.labels, keep_prob: 1.0&#125;)print(&apos;test accuracy %g&apos; % test_accuracy)sess.close() 参考：http://blog.csdn.net/u011974639/article/details/75363565#卷积神经网络简介]]></content>
  </entry>
  <entry>
    <title><![CDATA[反向传播神经网络（back propagation）]]></title>
    <url>%2F2017%2F10%2F14%2F%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88Back%20propagation%EF%BC%89%2F</url>
    <content type="text"><![CDATA[反向传播神经网络（Back propagation）BP神经网络是一种多层的前馈神经网络，通过误差逆向传播算法来不断调整网络参数，使网络的误差达到最小，是目前应用最广泛的神经网络。 BP神经网络模型神经网络由很多层构成，每一层又由许多单元组成。神经网络第一层叫输入层，最后一层叫输出层，中间的各层叫隐藏层。训练一个BP神经网络，实质上就是调整网络的权重（W）和偏置（b）这两个参数。BP神经网络的训练过程分为两部分 前向传输，逐层波浪式的传递输出值。 逆向反馈，反向逐层调整权重和偏置。 前向传播算法$$ a^{(1)}=w^{(1)}x + b^{(1)} $$ $$ z^{(1)}=ReLU(a^{(1)}) $$ $$ a^{(2)}=w^{(2)}z^{(1)} + b^{(2)}$$ $$ z^{(2)}$$ 反向传播算法多分类（softmax）搭建神经网络 特征归一化（normalization）控制特征值区间，使之与数据集中其他特征的区间匹配，可加快训练速度。 线性归一化（Min-Max scaling）。将原始数据线性转换到[0,1]区间内，适用于灰度转换等不符从正态分布的数据。$X{norm}=\frac{X-X{min}}{X{max}-X{min}}$ 0均值标准化（Z-score standardization）。将原始数据归一化为0均值，方差为1的数据集，适用于聚类、分类算法中服从正态分布的数据。$z=\frac{x-\mu}{\sigma}$ 网络的层数理论已经证明，一个S型隐藏层加上一个线性输出层，能够逼近任何有理函数，增加层数可以进一步降低误差，提高精度，但同时也是网络复杂化。 隐层神经元的个数增加神经元个数在结构实现上要比增加网络层数简单得多。 神经元数太少时，网络学习能力低，训练迭代的次数也比较多，训练精度也不高。 神经元数太多时，网络学习能力高，精确度也更高，训练迭代的次数也大，可能会出现过拟合(over fitting)现象。 因此，隐层神经元个数的选取原则是：在能够解决问题的前提下，增加一两个神经元，以加快误差下降速度。 参数初始化 全零初始化。导致网络中每个神经元都得到同样的输出，在反向传播中计算出同样的梯度，从而进行同样的参数更新。神经元之间就失去了不对称性的源头。 随机初始化。将权重初始化为很小的数值，以此来打破对称性。 12# n1为前一层神经元数，n2为本层神经元数w = np.random.randn(n1,n2) * 0.01 使用校准方差。随着输入数据量的增长，随机初始化的神经元的输出数据的分布中的方差也在增大。通过除以输入数据量的平方根来调整其数值范围，将神经元输出的方差就归一化。 1234# n1为前一层神经元数，n2为本层神经元数w = np.random.randn(n1,n2) * sqrt(2/n1)#ReLU神经元w = np.random.randn(n1,n2) * sqrt(2/n1)#tanh神经元w = np.random.randn(n1,n2) * sqrt(2/(n1 + n2))#最新的做法 学习速率 固定的学习速率。如果学习速率太小，则会使收敛过慢，如果学习速率太大，则会导致代价函数振荡。 学习率衰减。对于复杂的网络，在不同的阶段设置不同大小的学习速率，效果会更好。① $\alpha=\frac{1}{1+decay^{epoch}} \alpha_{0}$ ② $\alpha=0.95^{epoch} \alpha_{0}$ Batch Norm？？？？？？？？在网络的每一层输入的时候，又插入了一个归一化层，也就是先做一个归一化处理，然后再进入网络的下一层。 http://blog.csdn.net/hjimce/article/details/50866313 神经网络中的正则化正则化是以增大训练误差为代价来减少泛化误差的机器学习策略，主要是为了解决过拟合问题。现有的正则化策略中，有些是向模型添加额外的约束，如增加对参数的限制。有些是向目标函数增加惩罚项，对应于参数值的软约束。这些约束和惩罚实质上是为特定模型添加先验知识，以降低训练数据的影响，提高泛化能力。 正则化L1、L2 dropout正则化（dropout regularization）在神经网络的训练过程中，按照一定的概率，将部分神经元暂时关闭。经过交叉验证发现，dropout率等于0.5的时候效果最好，此时网络结构最多。 数据增强（Data Augmentation）通过图像的几何变换, 使用一种或多种组合数据增强变换来增加输入数据的量。 旋转|反射变换(Rotation/reflection): 随机旋转图像一定角度; 改变图像内容的朝向; 翻转变换(flip): 沿着水平或者垂直方向翻转图像; 缩放变换(zoom): 按照一定的比例放大或者缩小图像; 平移变换(shift): 在图像平面上对图像以一定方式进行平移; 早期停止法（early stopping）在每一个epoch结束时计算验证集的准确率，当准确率不再提高时，就停止训练。可以通过学习曲线来判断。 神经网络优化算法 mini-batch当数据量太大时，使用BGD会很慢。 指数加权平均 momentum RMStop Adam 梯度消失与梯度爆炸？？？？？梯度不稳定的原因在于BP网络自身。BP神经网络基于反向传播，在计算每层梯度时会涉及连乘操作。当网络过深时，如果连乘的因子小于1，乘积可能趋于0；如果连乘的因子大于1，乘积可能趋于无穷。解决梯度消失问题：不使用sigmoid函数。sigmoid函数的最大梯度仅为1/4，连乘很容易造成梯度消失。解决梯度爆炸问题：①学习率衰减。在每一层使用不同的学习速率，使每一层的乘积可以维持稳定。②梯度截断。在应用梯度之前先修饰数值，有助于确保数值稳定性，防止梯度爆炸出现。 BP神经网络的缺陷： 陷入局部极值，使训练失败。BP算法为一种局部搜索的优化方法，但它要解决的问题为求解复杂非线性函数的全局极值。 训练次数多使得学习效率低，收敛速度慢。 训练时学习新样本有遗忘旧样本的趋势。 参考：http://blog.csdn.net/lujiandong1/article/details/53320174http://blog.csdn.net/hjimce/article/details/50866313]]></content>
  </entry>
  <entry>
    <title><![CDATA[机器学习经典面试问题]]></title>
    <url>%2F2017%2F10%2F14%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%BB%8F%E5%85%B8%E9%9D%A2%E8%AF%95%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[宏观问题 看过哪些资料书，有什么想法 做过的项目，讲一讲 觉得自己突出的地方 基础问题 训练集，验证集，测试集的划分和作用 损失函数的种类，及其优化方法 BGD SGD Adam 过拟合与欠拟合 L1、L2：优化方法，坐标下降，LARS角回归 项目 特征相关性 数据如何进行预处理 处理数据缺失值 特征提取方法，判断特征是否重要 如何调节模型参数 聚类算法，无法度量，kd树 BP神经网络 CNN RNN，长时依赖问题及解决 决策树C4.5,ID3，防止过拟合 SVM原问题与对偶问题，kkt,过拟合 LR 各种算法应用场景 算法与数据结构 链表逆序 有序数组的交集 随机数1-5如何输出1-7 链表有环 连续乘机，溢出 面试注意要点 主动输出]]></content>
  </entry>
  <entry>
    <title><![CDATA[机器学习中的偏差与方差]]></title>
    <url>%2F2017%2F09%2F27%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E5%81%8F%E5%B7%AE%E4%B8%8E%E6%96%B9%E5%B7%AE%2F</url>
    <content type="text"><![CDATA[泛化误差模型的泛化能力（generalization ability）是指模型对未知数据的预测能力。同理，泛化误差（generalization error）是指模型对未知数据预测的误差。 泛化误差可分为：随机误差（random error）、偏差（bias）和方差（variance）。其中，随机误差是数据本身的噪声带来的，无法避免。 本文主要讲偏差和方差。 偏差与方差偏差表示参数与真实参数之间的误差。$bias(\hat{\theta}{m})=\mathcal{E}(\hat{\theta}{m})-\theta$当模型做出与实际情况不符的假设时就会引起错误（通常是模型太简单），就会导致偏差大，即对数据欠拟合。 方差表示模型在拟合不同的数据时，参数的变化程度。$Var(\hat{\theta})$​在理想情况下，同一来源的数据应满足独立同分布。那么针对同一问题，无论使用哪个数据集，所建立的模型应当是相同的。从公式的角度来说，就是目标方程的系数和常数项应当是近似相同的。但如果模型太复杂，那么随着训练数据集的不同，参数变化太大时，就会导致方差大，即对数据过拟合。 下图能够很好地解释偏差和方差。 在一个实际系统中，偏差与方差往往不能兼得。如果注重模型在训练数据上的准确度，忽视对模型的先验知识，这样可以减少模型的偏差，但会降低模型的泛化能力，造成方差增大。如果注重对模型的先验知识，对模型增加更多的限制，这样可以降低模型的方差，但也会降低模型在训练数据上的准确度，造成偏差增大。 Trade-Off方差与偏差之间的trade-off是机器学习的重要主题。 模型误差是偏差与方差的加和，因此可以绘制下图。 从图中可以看出 当模型太简单时，拟合能力不够强，偏差比较大；由于拟合能力不强, 数据集的扰动也无法使模型产生显著变化，方差较小。模型欠拟合。 随着模型复杂度提升，拟合能力逐渐增强，预测渐渐准确，偏差持续减小；数据的扰动也能够渐渐被学习器学到，方差增大。模型较优。 当模型太复杂时，拟合能力非常强，偏差很小；数据的轻微扰动都会导致模型发生显著变化，方差很大。此时，训练数据自身的、非全局的特性被模型捕捉到，模型过拟合。 Total Error曲线的拐点，是模型的最优位置。 将数据分为训练集和验证集，可以绘制出下图。 从图中可以看出 当模型太简单时，对训练集和验证集的预测误差较大，模型高偏差，欠拟合。 随着模型复杂度提升，拟合能力增强，对训练集和验证集的预测误差都减小，模型较优。 当模型太复杂时，对训练集过分拟合，训练误差进一步减小。然而泛化能力不高，验证集误差较大，模型高方差，过拟合。 如何处理欠拟合和过拟合参考吴恩达教授提供的处理模型欠拟合与过拟合的一般方法。 当模型处于欠拟合状态时，应该增加模型的复杂度。一般办法有： 增加模型迭代次数。 提高模型复杂度：例如在神经网络中增加神经网络层数、在SVM中用非线性SVM（核技术）代替线性SVM。 增加特征：特征少，对模型信息的刻画就不足够了 降低正则化权重：正则化为了限制模型的复杂度而设定的，降低其权值可以在模型训练中增加模型复杂度。 当模型处于过拟合状态时，应该降低模型的复杂度。一般方法有： 增加数据： 减少特征：PCA 正则化： 参考：http://scott.fortmann-roe.com/docs/BiasVariance.htmlhttps://plushunter.github.io/]]></content>
  </entry>
  <entry>
    <title><![CDATA[神经网络的正则化]]></title>
    <url>%2F2017%2F09%2F26%2F%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E6%AD%A3%E5%88%99%E5%8C%96%E5%8F%8A%E4%BC%98%E5%8C%96%2F</url>
    <content type="text"><![CDATA[机器学习监督问题的主要思想是‘minimize your error while regularizing your parameters’，即在最小化误差的同时正则化参数。 最小化误差是为了让模型拟合数据，即减少偏差；正则化参数是为了防止模型过分拟合数据，即减少方差。 什么是正则化正则化是用来解决过拟合问题 目标函数#]]></content>
  </entry>
  <entry>
    <title><![CDATA[机器学习激活函数(Activation Function)]]></title>
    <url>%2F2017%2F09%2F26%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0(Activation%20Function)%2F</url>
    <content type="text"><![CDATA[在机器学习中，尤其是神经网络中，会经常用到激活函数。本篇总结常见的激活函数及其优缺点。 激活函数的性质: 非线性：激活函数一般是非线性的，用来给网络加入非线性因素，增强网络的表达能力。如果使用线性激活函数，那么线性的组合仍然是线性。即，无论网络中有多少层神经元，其结果都相当于一层神经元。 可微性：当优化方法是基于梯度的时候，这个性质是必须的。 常见的激活函数常见的激活函数有：sigmoid函数 tanh函数 ReLU函数 Leaky-ReLU函数 sigmoid函数数学形式 $$sigmoid(x) = \frac{1}{1+e^{-x}}$$ 形状 /markdown-img-paste-20170926080536411.png) 描述 结果为[0,1]，均值为0.5。 优点 结果为[0,1]。在二元分类中，常用来做输出层的激活函数。 缺点 软饱和。当输入极大或者极小时，神经元的梯度是接近于0，处于饱和状态，网络变得很难学习。 偏移现象。均值为0.5，下一层神经元的输入均值不为0。 tanh函数数学形式$$tanh(x)=2sigmoid(2x)-1=\frac{2}{1+e^{-2x}}-1=\frac{e^{x}-e^{-x}}{e^{x}+e^{-x}}$$ 形状 /markdown-img-paste-20170926080437430.png) 描述 结果为[-1,1]，均值为0。 优点 均值为0。不存在偏移现象。 缺点 软饱和。当输入极大或者极小时，神经元的梯度是接近于0，处于饱和状态，网络变得很难学习。 ReLU函数数学形式$$ReLU(x) = max(0,x)$$ 形状 /markdown-img-paste-20170926091252946.png) 描述 结果为[0,∞]。 优点 计算速度快。ReLU只需要一个阈值就可以得到激活值，计算简单。 收敛速度快。在大于0的区域其梯度恒为1，不存在饱和现象。 缺点 偏移现象。均值不为0。 硬饱和。在小于0的区域其梯度为0。 神经元死亡。当输入落入小于0的区域时，其梯度等于0，导致对应权重无法更新。减小学习速率可以缓解。 Leaky-ReLU函数数学形式 $Leaky-ReLU(x) = max(\alpha x,x),\alpha = 0.01$ 形状 /markdown-img-paste-20170926091702440.png) 描述 结果为[-∞,∞]。 优点 计算速度快。ReLU只需要一个阈值就可以得到激活值，计算简单。 收敛速度快。在大于0的区域其梯度恒为1，不存在饱和现象。 解决了神经元死亡问题。在小于0的区域，存在梯度不为0。 缺点 不够成熟，不常用。 如何选择 sigmoid函数常用于二元分类神经网络的输出层。 ReLU函数常用于神经网络的隐藏层，使用时应注意将learning-rate设置小一点，避免神经元死亡。 硬饱和与软饱和 硬饱和。函数的导数等于0 软饱和。函数的导数趋近于0]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>激活函数</tag>
        <tag>sigmoid</tag>
        <tag>tanh</tag>
        <tag>ReLU</tag>
      </tags>
  </entry>
</search>
