<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[机器学习中的偏差与方差]]></title>
    <url>%2F2017%2F09%2F27%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E5%81%8F%E5%B7%AE%E4%B8%8E%E6%96%B9%E5%B7%AE%2F</url>
    <content type="text"><![CDATA[泛化误差模型的泛化能力（generalization ability）是指模型对未知数据的预测能力。同理，泛化误差（generalization error）是指模型对未知数据预测的误差。 泛化误差可分为：随机误差（random error）、偏差（bias）和方差（variance）。其中，随机误差是数据本身的噪声带来的，无法避免。 本文主要讲偏差和方差。 偏差与方差偏差表示参数与真实参数之间的误差。$bias(\hat{\theta}{m})=\mathcal{E}(\hat{\theta}{m})-\theta$当模型做出与实际情况不符的假设时就会引起错误（通常是模型太简单），就会导致偏差大，即对数据欠拟合。 方差表示模型在拟合不同的数据时，参数的变化程度。$Var(\hat{\theta})$​在理想情况下，同一来源的数据应满足独立同分布。那么针对同一问题，无论使用哪个数据集，所建立的模型应当是相同的。从公式的角度来说，就是目标方程的系数和常数项应当是近似相同的。但如果模型太复杂，那么随着训练数据集的不同，参数变化太大时，就会导致方差大，即对数据过拟合。 下图能够很好地解释偏差和方差。 在一个实际系统中，偏差与方差往往不能兼得。如果注重模型在训练数据上的准确度，忽视对模型的先验知识，这样可以减少模型的偏差，但会降低模型的泛化能力，造成方差增大。如果注重对模型的先验知识，对模型增加更多的限制，这样可以降低模型的方差，但也会降低模型在训练数据上的准确度，造成偏差增大。 Trade-Off方差与偏差之间的trade-off是机器学习的重要主题。 模型误差是偏差与方差的加和，因此可以绘制下图。 从图中可以看出 当模型太简单时，拟合能力不够强，偏差比较大；由于拟合能力不强, 数据集的扰动也无法使模型产生显著变化，方差较小。模型欠拟合。 随着模型复杂度提升，拟合能力逐渐增强，预测渐渐准确，偏差持续减小；数据的扰动也能够渐渐被学习器学到，方差增大。模型较优。 当模型太复杂时，拟合能力非常强，偏差很小；数据的轻微扰动都会导致模型发生显著变化，方差很大。此时，训练数据自身的、非全局的特性被模型捕捉到，模型过拟合。 Total Error曲线的拐点，是模型的最优位置。 将数据分为训练集和验证集，可以绘制出下图。 从图中可以看出 当模型太简单时，对训练集和验证集的预测误差较大，模型高偏差，欠拟合。 随着模型复杂度提升，拟合能力增强，对训练集和验证集的预测误差都减小，模型较优。 当模型太复杂时，对训练集过分拟合，训练误差进一步减小。然而泛化能力不高，验证集误差较大，模型高方差，过拟合。 如何处理欠拟合和过拟合参考吴恩达教授提供的处理模型欠拟合与过拟合的一般方法。 当模型处于欠拟合状态时，应该增加模型的复杂度。一般办法有： 增加模型迭代次数。 提高模型复杂度：例如在神经网络中增加神经网络层数、在SVM中用非线性SVM（核技术）代替线性SVM。 增加特征：特征少，对模型信息的刻画就不足够了 降低正则化权重：正则化为了限制模型的复杂度而设定的，降低其权值可以在模型训练中增加模型复杂度。 当模型处于过拟合状态时，应该降低模型的复杂度。一般方法有： 增加数据： 减少特征：PCA 正则化： 参考：http://scott.fortmann-roe.com/docs/BiasVariance.htmlhttps://plushunter.github.io/]]></content>
  </entry>
  <entry>
    <title><![CDATA[神经网络的正则化]]></title>
    <url>%2F2017%2F09%2F26%2F%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E6%AD%A3%E5%88%99%E5%8C%96%E5%8F%8A%E4%BC%98%E5%8C%96%2F</url>
    <content type="text"><![CDATA[机器学习监督问题的主要思想是‘minimize your error while regularizing your parameters’，即在最小化误差的同时正则化参数。 最小化误差是为了让模型拟合数据，即减少偏差；正则化参数是为了防止模型过分拟合数据，即减少方差。 什么是正则化正则化是用来解决过拟合问题 目标函数#]]></content>
  </entry>
  <entry>
    <title><![CDATA[机器学习激活函数(Activation Function)]]></title>
    <url>%2F2017%2F09%2F26%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0(Activation%20Function)%2F</url>
    <content type="text"><![CDATA[在机器学习中，尤其是神经网络中，会经常用到激活函数。本篇总结常见的激活函数及其优缺点。 激活函数的性质: 非线性：激活函数一般是非线性的，用来给网络加入非线性因素，增强网络的表达能力。如果使用线性激活函数，那么线性的组合仍然是线性。即，无论网络中有多少层神经元，其结果都相当于一层神经元。 可微性：当优化方法是基于梯度的时候，这个性质是必须的。 常见的激活函数常见的激活函数有：sigmoid函数 tanh函数 ReLU函数 Leaky-ReLU函数 sigmoid函数数学形式 $$sigmoid(x) = \frac{1}{1+e^{-x}}$$ 形状 描述 结果为[0,1]，均值为0.5。 优点 结果为[0,1]。在二元分类中，常用来做输出层的激活函数。 缺点 软饱和。当输入极大或者极小时，神经元的梯度是接近于0，处于饱和状态，网络变得很难学习。 偏移现象。均值为0.5，下一层神经元的输入均值不为0。 tanh函数数学形式$$tanh(x)=2sigmoid(2x)-1=\frac{2}{1+e^{-2x}}-1=\frac{e^{x}-e^{-x}}{e^{x}+e^{-x}}$$ 形状 描述 结果为[-1,1]，均值为0。 优点 均值为0。不存在偏移现象。 缺点 软饱和。当输入极大或者极小时，神经元的梯度是接近于0，处于饱和状态，网络变得很难学习。 ReLU函数数学形式$$ReLU(x) = max(0,x)$$ 形状 描述 结果为[0,∞]。 优点 计算速度快。ReLU只需要一个阈值就可以得到激活值，计算简单。 收敛速度快。在大于0的区域其梯度恒为1，不存在饱和现象。 缺点 偏移现象。均值不为0。 硬饱和。在小于0的区域其梯度为0。 神经元死亡。当输入落入小于0的区域时，其梯度等于0，导致对应权重无法更新。减小学习速率可以缓解。 Leaky-ReLU函数数学形式$$Leaky-ReLU(x) = max(\alpha x,x),\alpha = 0.01$$ 形状 描述 结果为[-∞,∞]。 优点 计算速度快。ReLU只需要一个阈值就可以得到激活值，计算简单。 收敛速度快。在大于0的区域其梯度恒为1，不存在饱和现象。 解决了神经元死亡问题。在小于0的区域，存在梯度不为0。 缺点 不够成熟，不常用。 如何选择 sigmoid函数常用于二元分类神经网络的输出层。 ReLU函数常用于神经网络的隐藏层，使用时应注意将learning-rate设置小一点，避免神经元死亡。 硬饱和与软饱和 硬饱和。函数的导数等于0 软饱和。函数的导数趋近于0]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>激活函数</tag>
        <tag>sigmoid</tag>
        <tag>tanh</tag>
        <tag>ReLU</tag>
      </tags>
  </entry>
</search>
