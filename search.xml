<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[git的基本命令]]></title>
    <url>%2F2017%2F10%2F26%2Fgit%E7%9A%84%E5%9F%BA%E6%9C%AC%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[本地目录变成git仓库1git init 把工作区的修改提交到Git仓库的暂存区1git add file 把暂存区的所有内容提交到当前分支1git commit -m &quot;summary&quot; 查看当前分支修改状态1git status 远程把本地仓库添加远程库(GitHub)1git remote add origin git@github.com:yourname/repository.git 把本地库master的所有内容推送到远程库1git push origin master 从远程库克隆出一个本地库1git clone git@github.com:yourname/repository.git 从远程获取最新版本到本地1git fetch origin master:temp 比较本地的仓库和远程参考的区别1git diff temp 合并temp分支到master分支1git merge temp 删除此分支1git branch -d temp 移除远程库1git remote rm origin]]></content>
      <categories>
        <category>git</category>
      </categories>
      <tags>
        <tag>GitHub</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[经典卷积神经网络（AlexNet）]]></title>
    <url>%2F2017%2F10%2F21%2F%E7%BB%8F%E5%85%B8%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C(AlexNet)%2F</url>
    <content type="text"><![CDATA[写在前面不同的卷积神经网络的区别，其实就是网络的结构不同。本文介绍最经典的CNN——AlexNet。 AlexNet网络结/markdown-img-paste-20171017203240734.png)AlexNet总共包括8层，其中前5层卷积层，后面3层是全连接层（图中尺寸有问题）。| 操作层 | 输入尺寸 | 参数 | 输出尺寸 || :—— | :———- | :————————————- | :———- || 输入层 | [227x227x3] | | [227x227x3] || 卷积层1 | [227x227x3] | 核[11,11,3,96] strides=[1,4,4,1] | [55x55x96] || 池化层1 | [55x55x96] | 核[1,3,3,1] strides=[1,2,2,1] | [27x27x96] || 卷积层2 | [27x27x96] | 核[5,5,96,256] strides=[1,1,1,1] pad=2] | [27x27x256] || 池化层2 | [27x27x256] | 核[1,3,3,1] strides=[1,2,2,1] | [13x13x256] || 卷积层3 | [13x13x256] | 核[3,3,256,384] strides=[1,1,1,1] pad=1 | [13x13x384] || 卷积层4 | [13x13x384] | 核[3,3,384,384] strides=[1,1,1,1] pad=1 | [13x13x384] || 卷积层5 | [13x13x384] | 核[3,3,384,256] strides=[1,1,1,1] pad=1 | [13x13x256] || 池化层3 | [13x13x256] | 核[1,3,3,1] strides=[1,2,2,1] pad=1 | [6x6x256] || 全连接层1 | [6x6x256] | [4096] | [4096] || 全连接层2 | [4096] | [4096] | [4096] || softmax | [4096] | [1000] | [1000] | 特点 针对网络架构: 使用ReLU作为激活函数，收敛速度快，并验证其效果在较深的网络要优于Sigmoid。 使用LRN层，对局部神经元的活动创建竞争机制，使得其中响应比较大的值变的相对更大，并抑制其他反馈较小的神经元，增强了模型的泛化能力。 使用重叠的最大池化，让步长比池化核的尺寸小，池化层的输出之间会有重叠，提升了特征的丰富性。 针对过拟合现象: 数据增强，对原始图像随机截取输入图片尺寸大小(以及对图像作水平翻转操作)，使用数据增强后大大减轻过拟合，提升模型的泛化能力。 使用Dropout随机忽略一部分神经元，避免模型过拟合。 针对训练速度: 使用GPU。 AlexNet 应用 MNIST1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192# coding:utf8from tensorflow.examples.tutorials.mnist import input_dataimport tensorflow as tfdef initialize_weight(shape, stddev, name): initial = tf.truncated_normal(shape, dtype = tf.float32, stddev = stddev) return tf.Variable(initial, name = name)def initialize_bias(shape): initial = tf.random_normal(shape) return tf.Variable(initial)def conv2d(x, w, b): return tf.nn.relu((tf.nn.conv2d(x, w, strides = [1, 1, 1, 1], padding = &apos;SAME&apos;) + b))def max_pool(x, f): return tf.nn.max_pool(x, ksize = [1, f, f, 1], strides = [1, 1, 1, 1], padding = &apos;SAME&apos;)# Create AlexNet Modelx = tf.placeholder(tf.float32, [None, 784])x_image = tf.reshape(x, shape = [-1, 28, 28, 1])y = tf.placeholder(tf.float32, [None, 10])# layer1: conv2dw1_conv = initialize_weight([3, 3, 1, 64], 0.1, &apos;w1&apos;)b1_conv = initialize_bias([64])h1_conv = conv2d(x_image, w1_conv, b1_conv)#28x28x1&gt;&gt;28x28x64h1_pool = max_pool(h1_conv, 2)#28x28x64&gt;&gt;14x14x64# layer2: conv2dw2_conv = initialize_weight([3, 3, 64, 64], 0.1, &apos;w2&apos;)b2_conv = initialize_bias([64])h2_conv = conv2d(h1_pool, w2_conv, b2_conv)#14x14x64&gt;&gt;14x14x64h2_pool = max_pool(h2_conv, 2)#14x14x64&gt;&gt;7x7x64# layer3: conv2dw3_conv = initialize_weight([3, 3, 64, 128], 0.1, &apos;w3&apos;)b3_conv = initialize_bias([128])h3_conv = conv2d(h2_pool, w3_conv, b3_conv)#7x7x64&gt;&gt;7x7x128# layer4: conv2dw4_conv = initialize_weight([3, 3, 128, 128], 0.1, &apos;w4&apos;)b4_conv = initialize_bias([128])h4_conv = conv2d(h3_conv, w4_conv, b4_conv)#7x7x128&gt;&gt;7x7x128# layer5: conv2dw5_conv = initialize_weight([3, 3, 128, 256], 0.1, &apos;w5&apos;)b5_conv = initialize_bias([256])h5_conv = conv2d(h4_conv, w5_conv, b5_conv)#7x7x128&gt;&gt;7x7x256h5_pool = max_pool(h5_conv, 2)#shape = h5_pool.get_shape()h5_pool_flat = tf.reshape(h5_pool, [-1, shape[1].value*shape[2].value*shape[3].value])# layer6: full connectionw6_fc = initialize_weight([256*28*28, 1024], 0.01, &apos;w6&apos;)b6_fc = initialize_bias([1024])h6_fc = tf.nn.relu(tf.matmul(h5_pool_flat, w6_fc) + b6_fc)keep_prob = tf.placeholder(&apos;float&apos;)h6_drop = tf.nn.dropout(h6_fc, keep_prob = keep_prob)# layer7: full connectionw7_fc = initialize_weight([1024, 1024], 0.01, &apos;w7&apos;)b7_fc = initialize_bias([1024])h7_fc = tf.nn.relu(tf.matmul(h6_drop, w7_fc) + b7_fc)h7_drop = tf.nn.dropout(h7_fc, keep_prob = keep_prob)# layer8: softmaxw8_sf = initialize_weight([1024, 10], 0.01, &apos;w8&apos;)b8_sf = initialize_bias([10])y_conv = tf.nn.softmax(tf.matmul(h7_drop, w8_sf) + b8_sf)cross_entropy = -tf.reduce_sum(y * tf.log(y_conv))train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y, 1))accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))# Train AlexNet Modelmnist = input_data.read_data_sets(&apos;MNIST_data/&apos;, one_hot = True)sess = tf.Session()sess.run(tf.global_variables_initializer())for i in range(1500): batch = mnist.train.next_batch(64) if i % 10 == 0: train_accuracy = sess.run(accuracy, feed_dict = &#123;x: batch[0], y: batch[1], keep_prob: 0.9&#125;) print(&apos;step %d, training accuracy %g&apos; % (i, train_accuracy)) sess.run(train_step, feed_dict = &#123;x: batch[0], y: batch[1], keep_prob: 0.9&#125;)test_accuracy = sess.run(accuracy, feed_dict = &#123;x: mnist.test.images, y: mnist.test.labels, keep_prob: 0.9&#125;)print(&apos;test accuracy %g&apos; % test_accuracy)sess.close() 参考：http://blog.csdn.net/u011974639/article/details/76146822#alexnet的特点]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>CNN</tag>
        <tag>MNIST</tag>
        <tag>AlexNet</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[卷积神经网络(Convolutional Neural Network）]]></title>
    <url>%2F2017%2F10%2F19%2F%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C(Convolutional%20Neural%20Network)%2F</url>
    <content type="text"><![CDATA[卷积神经网络卷积神经网络 (Convolutional Neural Network, CNN)，是一种专门用来处理具有网格结构数据的神经网络。例如时间序列数据(时间轴上的一维网格)和图像数据(二维的像素网格)。 CNN作为一个深度学习架构被提出的最初诉求，是降低对图像数据预处理的要求，以及避免复杂的特征工程。CNN可以直接使用图像作为输入，减少了许多特征提取的手续。CNN的最大特点在于卷积的权值共享结构，大幅度减少神经网络的参数，同时防止了过拟合。 卷积神经网络结构）/markdown-img-paste-2017101621075175.png) 一般来说，卷积神经网络主要由以下结构组成: 输入层输入层是整个神经网络的输入，一般为图片的像素矩阵(一般为三维矩阵，即像素x像素x通道) 卷积层卷积层的作用是从输入层中抽象出更高的特征。一般流程如下： 图像通过多个不同的卷积核滤波，并添加偏置(bias)，提取局部特征，每一个卷积核会映射出一个新的2D图像。 将前面卷积核的滤波输出结果，进行非线性的激活函数处理。 对激活函数的结构再进行池化操作(降采样），目前一般使用最大池化，保留最大特征，提示模型的畸变容忍能力。 卷积图像处理中的卷积计算，依赖于卷积核Kernel，而卷积核就是一个滤波器，卷积核与输入信号做加权叠加得到输出。 /markdown-img-paste-20171016211502468.png)/20170719111641148.gif) 卷积参数（以上图为例） 输入尺寸4x4 卷积核尺寸3x3 输出尺寸2x2 移动步长（stride）1 可以发现，只要卷积核的尺寸不是1x1,那么输出尺寸必定会小于输入尺寸大小。而在实际图片处理过程中，很多时候需要保持图像的大小不变。为了保持输入和输出尺寸一致，同时也有卷积的效果，我们对输入图片的外圈做填充操作。 因此，引入新的参数padding，即在输入的外圈补零的圈数(数字零对卷积贡献为零,不会产生额外的偏差)。 /20170719114528262.gif) 这里给出输入尺寸、输出尺寸、滤波器尺寸、stride和padding的关系: 输入图片的尺寸大小W1 x H1 卷积核(又称滤波器，以下都称滤波器)的大小F x F stride：S padding:P 输出图片的尺寸大小W2 x H2 $W2 = (W1-F+2P)/S + 1$ $H2 = (H1-F+2P)/S + 1$ 权值共享每一个卷积层中使用的过滤器参数是相同的，这就是卷积核的权值共享。 从直观上理解，共享卷积核可以使得图像上的内容不受位置的影响，这提高了模型对平移的容忍性，这大大的提高了模型提取特征的能力。 从网络结构上来说，共享每一个卷积层的卷积核，可以大大减少网络的参数，这不仅可以降低计算的复杂度，而且还能减少因为连接过多导致的严重过拟合，从而提高模型的泛化能力。 权值共享与传统的全连接的区别如下图： /markdown-img-paste-20171016214052344.png) 多卷积核在每一个卷积层中，会使用多个卷积核运算。这是因为每一个卷积核滤波得到的图像就是一类特征的映射，使用多个卷积核，就能提供多个方向上的图像特征，可以从图像中抽象出有效而丰富的高阶特征。 /markdown-img-paste-20171016214315791.png) 池化层池化层可以非常有效地缩小图片的尺寸，减少最后全连接层的参数，保留最显著的特征，提升模型的畸变容忍能力，防止过拟合。 池化层前向传播过程类似于卷积层，是通过移动一个类似滤波器的结构完成的。与卷积层不同的是，池化层的滤波器计算不是加权求和，而且求区域内的极大值或者平均值。 最大池化层(max pooling)，最常使用，计算图像区域内最大值，提取纹理效果较好。 /markdown-img-paste-20171016214730544.png) 平均池化层(mean pooling)，计算图像区域的平均值，保留背景更好。 /markdown-img-paste-20171016214806523.png) 随机池化层(stochastic pooling)，介于两者之间，通过对像素点按照数值大小赋予概率，再按照概率进行亚采样。 全连接层图像中被抽象成了信息含量更高的特征在经过神经网络完成后续分类等任务。 输出层一般是使用softmax输出概率值或者分类结果。 TensorFlow部署CNN卷积通道混合处理的任意滤波器1tf.nn.conv2d(x, W, strides, padding) strides = [batch, width, height, channels] batch = 1表示样本一个一个遍历 width height = 1表示移动的步长 channels = 1表示通道一个一个滑动 padding = ‘SAME’,’VALID’ ‘SAME’ 输入输出尺寸相同 ‘VALID’ (w-f)/k + 1 池化1tf.nn.max_pool(x, ksize, strides, padding) ksize = [1, height, width, 1] batch = 1表示不在样本上做池化 width height表示池化窗口大小 channels = 1表示不在通道上做池化 激活函数12345tf.nn.relu(x)tf.nn.dropout(x, keep_prob)tf.nn.bias_add(x, b)tf.sigmoid(x)tf.tanh(x) CNN 应用 MNIST12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970from tensorflow.examples.tutorials.mnist import input_dataimport tensorflow as tfdef initialize_weight(shape): initial = tf.truncated_normal(shape, stddev = 0.1) return tf.Variable(initial)def initialize_bias(shape): initial = tf.constant(0.1, shape = shape) return tf.Variable(initial)def conv2d(x, W): return tf.nn.conv2d(x, W, strides = [1, 1, 1, 1], padding = &apos;SAME&apos;)def max_pool_2x2(x): return tf.nn.max_pool(x, ksize = [1, 2, 2, 1], strides = [1, 2, 2, 1], padding = &apos;SAME&apos;)#create CNN Modelx = tf.placeholder(&apos;float&apos;, [None, 784])y = tf.placeholder(&apos;float&apos;, [None, 10])x_image = tf.reshape(x, [-1,28,28,1])#28x28x1# layer1:conv2dW1_conv = initialize_weight([5, 5, 1, 32])b1_conv = initialize_bias([32])h1_conv = tf.nn.relu(conv2d(x_image, W1_conv) + b1_conv)#28x28x1&gt;&gt;28x28x32h1_pool = max_pool_2x2(h1_conv)#28x28x32&gt;&gt;14x14x32# layer2:conv2dW2_conv = initialize_weight([5, 5, 32, 64])b2_conv = initialize_bias([64])h2_conv = tf.nn.relu(conv2d(h1_pool, W2_conv) + b2_conv)#14x14x32&gt;&gt;14x14x64h2_pool = max_pool_2x2(h2_conv)#14x14x64&gt;&gt;7x7x64h2_pool_flat = tf.reshape(h2_pool, [-1, 7*7*64])#7x7x64&gt;&gt;7*7*64# layer3:full connectionW3_fc = initialize_weight([7*7*64, 1024])b3_fc = initialize_bias([1024])h3_fc = tf.nn.relu(tf.matmul(h2_pool_flat, W3_fc) + b3_fc)#7*7*64&gt;&gt;1024# drop outkeep_prob = tf.placeholder(&apos;float&apos;)h3_fc_drop = tf.nn.dropout(h3_fc, keep_prob)# layer4: full connectionW4_fc = initialize_weight([1024, 10])b4_fc = initialize_bias([10])y_conv = tf.nn.softmax(tf.matmul(h3_fc_drop, W4_fc) + b4_fc)cross_entropy = -tf.reduce_sum(y*tf.log(y_conv))train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y,1))accuracy = tf.reduce_mean(tf.cast(correct_prediction, &apos;float&apos;))# train modelmnist = input_data.read_data_sets(&apos;MNIST_data/&apos;, one_hot = True)sess = tf.Session()sess.run(tf.global_variables_initializer())for i in range(20000): batch = mnist.train.next_batch(50) if i % 100 == 0: train_accuracy = sess.run(accuracy, feed_dict = &#123;x: batch[0], y: batch[1], keep_prob: 1.0&#125;) print(&apos;step %d, training accuracy %g&apos; % (i, train_accuracy)) sess.run(train_step, feed_dict = &#123;x: batch[0], y: batch[1], keep_prob: 0.5&#125;)test_accuracy = sess.run(accuracy, feed_dict = &#123;x: mnist.test.images, y: mnist.test.labels, keep_prob: 1.0&#125;)print(&apos;test accuracy %g&apos; % test_accuracy)sess.close() 参考：http://blog.csdn.net/u011974639/article/details/75363565#卷积神经网络简介]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>CNN</tag>
        <tag>MNIST</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度模型中的优化]]></title>
    <url>%2F2017%2F10%2F17%2F%E6%B7%B1%E5%BA%A6%E6%A8%A1%E5%9E%8B%E4%B8%AD%E7%9A%84%E4%BC%98%E5%8C%96%2F</url>
    <content type="text"><![CDATA[神经网络优化中的挑战局部极小值是否存在大量偏差严重的局部极小值，优化算法是否会碰到这些局部极小值，都是尚未解决的公开问题。 悬崖与梯度爆炸多层神经网络通常有像悬崖一样的斜率较大区域，这是由于几个较大的权重相乘导致的。遇到斜率极大的悬崖结构时，梯度更新会很大程度地改变参数值，通常会跳过这类悬崖结构。 不管我们是从上还是从下接近悬崖，情况都很糟糕，但幸运的是可以用使用启发式梯度截断 (gradient clipping) 来避免其主要缺点。基本想法源自考虑到梯度并没有指明最佳步长，只是说明在无限小区域内的最佳方向。当传统的梯度下降算法提议更新很大一步时，启发式梯度截断会干涉去减小步长，从而使其不太可能走出梯度很大的悬崖区域。悬崖结构在循环神经网络的损失函数中很常见，因为这类模型会涉及到多个时间步长因素的相乘。 长期依赖当计算图变得极深时，神经网络优化算法会面临的另外一个难题就是长期依赖问题——由于变深的结构使模型丧失了学习到先前信息的能力，让优化变得极其困难。 梯度消失与梯度爆炸梯度消失与梯度爆炸是指该计算图上的梯度也会因为网络过深而大幅度变化。消失的梯度使得难以知道参数朝哪个方向移动能够改进损失函数，而爆炸梯度会使得学习不稳定。悬崖结构便是爆炸梯度现象的一个例子。 梯度不稳定的原因在于网络本身，在计算每层梯度时会涉及连乘操作。当网络过深时，如果连乘的因子小于1，乘积可能趋于0；如果连乘的因子大于1，乘积可能趋于无穷。解决梯度消失问题：不使用sigmoid函数。sigmoid函数的最大梯度仅为1/4，连乘很容易造成梯度消失。解决梯度爆炸问题：①学习率衰减。在每一层使用不同的学习速率，使每一层的乘积可以维持稳定。②梯度截断。在应用梯度之前先修饰数值，有助于确保数值稳定性，防止梯度爆炸出现。 基本算法批量梯度下降(Batch Gradient Descend)批量梯度下降(BGD)在计算参数时，使用整个训练集，沿着整个训练集的梯度方向下降。$$\theta{\text{ML}}=\arg\max\limits{\theta}\sum_{i=1}^{m}\log p(x^{(i)},y^{(i)};\theta)$$当数据量太大时，计算速度会很慢。BGD使用固定的学习速率。 随机梯度下降(Stochastic Gradient Descend)随机梯度下降(SGD)按照数据生成分布抽取m个小批量(独立同分布)的mini-batch样本，沿着随机挑选的mini-batch数据的梯度方向下降。 SGD算法中的一个关键参数是学习速率。在实践中，有必要随着时间的推移逐渐降低学习速率，我们将第 k 步迭代的学习速率记作 $\epsilon_{k}$。 实践中，一般会线性衰减学习率直到第$\tau$次迭代:$$\epsilon=(1-\alpha)\epsilon{0}+\alpha\epsilon{\tau},\alpha=\frac{k}{\tau}$$在$\tau$步迭代之后，一般使$\tau$保持常数。参数选择为 $\epsilon{0}$，$\epsilon{\tau}$，$\tau$。通常$\tau$被设为需要反复遍历训练样本几百次的迭代次数。通常$\epsilon{\tau}$应设为大于1% 的$\epsilon{0}$。主要问题是如何设置$\epsilon{0}$。若$\epsilon{0}$太大，学习曲线将会剧烈振荡，损失函数值通常会明显增加。温和的振荡是良好的，特别是训练于随机损失函数上。如果学习速率太慢，那么学习进程会缓慢。如果初始学习速率太低，那么学习可能会卡在一个相当高的损失值。通常，就总训练时间和最终损失值而言，最优初始学习速率会高于大约迭代 100 步后输出最好效果的学习速率。因此，通常最好是检测最早的几轮迭代，使用一个高于此时效果最佳学习速率的学习速率，但又不能太高以致严重的不稳定性。 动量(Momentum)随机梯度下降(SGD)方法中的高方差振荡使得网络很难稳定收敛，所以有研究者提出了一种称为动量的技术，通过优化相关方向的训练和弱化无关方向的振荡，来加速SGD训练。 动量算法引入了变量$v$充当速度的角色——它代表参数在参数空间移动的方向和速度。 更新规则如下$$v\gets\alpha v-\epsilon\nabla{\theta}\lgroup\frac{1}{m}\sum{i=1}^{m}\mathcal{L}(f(x^{(i)};\theta),y^{(i)})\rgroup\\theta\gets\theta+v$$速度$v​$累积了梯度元素$\nabla{\theta}\lgroup\frac{1}{m}\sum{i=1}^{m}\mathcal{L}(f(x^{(i)};\theta),y^{(i)})\rgroup​$。$\alpha​$ 越大，之前梯度对现在方向的影响也越大，$\alpha​$通常设定为0.9。在参数更新过程中，使网络能更优和更稳定的收敛， 减少振荡过程。 不带动量的SGD与带动量的SGD比较如下： 一个条件数较差的二次目标函数看起来像一个长而窄的山谷或陡峭的峡谷。可以看出，带动量的梯度下降能够正确地纵向穿过峡谷，而不带动量的梯度下降则会浪费时间在峡谷的窄轴上来回移动。 Nesterov 动量Nesterov 动量可以解释为在标准动量方法中添加了一个校正因子。$$v\gets\alpha v\gets\epsilon\nabla{\theta}\lgroup\frac{1}{m}\sum{i=1}^{m}\mathcal{L}(f(x^{(i)};\theta+\alpha v),y^{(i)})\rgroup\\theta\gets\theta+v$$ 自适应学习率算法AdaGradAdaGrad算法能够独立地适应所有模型参数的学习速率，放缩每个参数反比于其所有梯度历史平方值总和的平方根。具有损失最大偏导的参数相应地有一个快速下降的学习速率，而具有小偏导的参数在学习速率上有相对较小的下降。净效果是在参数空间中更为平缓的倾斜方向会取得更大的进步。 AdaGrad算法具有一些令人满意的理论性质。然而，经验上已经发现，对于训练深度神经网络模型而言，从训练开始时积累梯度平方会导致有效学习速率过早和过量的减小。AdaGrad 在某些深度学习模型上效果不错，但不是全部。 RMSPropRMSProp算法修改 AdaGrad 以在非凸设定下效果更好，改变梯度积累为指数加权的移动均值。AdaGrad 根据平方梯度的整个历史收缩学习率，可能使得学习率在达到这样的凸结构前就变得太小了。RMSProp 使用指数衰减平均以丢弃遥远过去的历史，使其能够在找到凸碗状结构后快速收敛，它就像一个初始化于该碗状结构的 AdaGrad 算法实例。 ​ AdamAdam将动量直接并入了梯度一阶矩（指数加权）的估计，其次，Adam包括偏置修正，修正从原点初始化的一阶矩（动量项）和（非中心的）二阶矩的估计。Adam通常被认为对超参数的选择相当鲁棒，尽管学习率有时需要从建议的默认修改。 在实际应用中，Adam方法效果良好。与其他自适应学习率算法相比，其收敛速度更快，学习效果更为有效，而且可以纠正其他优化技术中存在的问题，如学习率消失、收敛过慢或是高方差的参数更新导致损失函数波动较大等问题。 选择正确的优化方法Adam在实际应用中效果良好，超过了其他的自适应技术。 如果输入数据集比较稀疏，SGD和动量项等方法可能效果不好。因此对于稀疏数据集，应该使用某种自适应学习率的方法，且另一好处为不需要人为调整学习率，使用默认参数就可能获得最优值。 如果想使训练深层网络模型快速收敛或所构建的神经网络较为复杂，则应该使用Adam或其他自适应学习速率的方法，因为这些方法的实际效果更优。 优化策略Batch Normalization网络训练过程中参数不断改变导致后续每一层输入的分布也发生变化，而学习的过程又要使每一层适应输入的分布，因此我们不得不降低学习率、小心地初始化。 批归一化(BN)即将某一层输出归一化，使得其均值为0方差为1。值得注意的是，BN是在channel维度做的，即将每个channel都进行归一化，如果有n个channel那么便会有n个归一化操作。具体来说如果某个层的输出为$x=(x^{0},x^{1},…,x^{n})$，那么$$\hat{x}^{k}=\frac{x^{k}-E[x^{k}]}{\sqrt{Var[x^{k}]}}$$但如果简单地这么干，会降低层的表达能力。比如下图，在使用sigmoid激活函数的时候，如果把数据限制到0均值单位方差，那么相当于只使用了激活函数中近似线性的部分，这显然会降低模型表达能力。 为此，又为BN增加了2个参数，用来保持模型的表达能力。 于是最后的输出为$$y^{k}=\gamma^{k}\hat{x}+\beta^{k}$$上述公式中用到了均值E和方差Var，需要注意的是理想情况下E和Var应该是针对整个数据集的，但显然这是不现实的。因此，用一个Batch的均值和方差作为对整个数据集均值和方差的估计。 BN可以加快网络收敛速度，十分好用。 参考 《Deeping Learning》]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>SGD</tag>
        <tag>Momentum</tag>
        <tag>Nesterov动量</tag>
        <tag>AdaGrad</tag>
        <tag>RMSProp</tag>
        <tag>Adam</tag>
        <tag>Batch Normlization</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度学习中的正则化]]></title>
    <url>%2F2017%2F10%2F15%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E6%AD%A3%E5%88%99%E5%8C%96%2F</url>
    <content type="text"><![CDATA[机器学习中的一个核心问题是设计不仅在训练数据集上表现好，而且能在新输入上泛化好的算法。在机器学习中，许多策略被显示地设计来减少测试误差（可能以增大训练误差为代价），这些策略被统称为正则化。 这些约束和惩罚实质上是为特定模型添加先验知识，以降低训练数据的影响，提高泛化能力。 参数范数惩罚许多正则化方式通过对目标函数$J$添加一个参数范数惩罚$\Omega(\theta)$，限制模型的学习能力。正则化后的目标函数表示为:$$\tilde{J}(\theta;X,y)=J(\theta;X,y)+\lambda\Omega(\theta)$$ 贝叶斯统计频率学派认为，参数$\theta$是未知的常量，我们的工作是利用极大似然法(maximum likelihood,ML)对参数$\theta$进行估计。$$\theta{\text{ML}}=\text{arg} \max\limits{\theta}\prod{i=1}^{m}p(y^{(i)}|x^{(i)};\theta)$$贝叶斯学派认为，参数$\theta$是未知的随机变量，可能服从某种先验概率(prior distribution)分布$p(\theta)$。对于一个训练集$S={x^{(i)},y^{(i)}},i=0,1,…n$ ，如果我们要对新的数据进行预测，我们可以通过贝叶斯公式算出θ的后验概率(posterior distribution)：$$p(\theta|S)=\frac{p(S|\theta)p(\theta)}{p(S)}\=\frac{(\prod{i=1}^{m}p(y^{(i)}|x^{(i)},\theta))p(\theta)}{\int{\theta}(\prod{i=1}^{m}p(y^{(i)}|x^{(i)},\theta)p(\theta))d\theta}\$$使用$\theta​$的后验分布对新的$x​$进行预测，有：$p(y|x,S)=\int_{\theta}p(y|x,\theta)p(\theta|S)d\theta​$。 上式要求对$\theta$进行积分，由于$\theta$往往是高维的，所以这很难实现。 因此在实际应用中我们常常近似$\theta$的后验概率，一种常用的近似方式就是一个点的估计来代替，最大后验概率(maximum a posteriori)估计如下：$$\theta{\text{MAP}}=\text{arg} \max\limits{\theta}\prod{i=1}^{m}p(y^{(i)}|x^{(i)},\theta)p(\theta)\= \text{arg} \max\limits{\theta}\sum{i=1}^{m}\log(p(y^{(i)}|x^{(i)},\theta)p(\theta))\= \text{arg} \max\limits{\theta}\sum{i=1}^{m}\log(p(y^{(i)}|x^{(i)},\theta))+\log(p(\theta))\=\text{arg} \min\limits{\theta}-\sum{i=1}^{m}\log(p(y^{(i)}|x^{(i)},\theta))-\log(p(\theta))\\text{arg} \min\limits{\theta}-\sum{i=1}^{m}\log(p(y^{(i)}|x^{(i)},\theta))-\sum{i=1}^{m}\log p(\theta_{i})$$上式中的$p(y^{(i)}|x^{(i)},\theta)​$取决于具体的学习模型。例如，当使用逻辑回归模型时，$p(y^{(i)}|x^{(i)},\theta)=[\pi(x^{(i)})]^{y^{(i)}}\cdot [1-\pi(x^{(i)})]^{1-y^{(i)}}​$。 综上所述，从贝叶斯的角度来讲，参数范数惩罚相当于对模型添加先验概率。 L2参数正则化L2参数正则化相当于参数$\theta$的先验概率满足正态分布，即$p(\theta{i})=\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(\theta{i}-\mu)^{2}}{2\sigma^{2}}}$。$$\theta{\text{MAP}}=\text{arg} \min\limits{\theta}-\sum{i=1}^{m}\log(p(y^{(i)}|x^{(i)},\theta))-\sum{i=1}^{m}\log p(\theta{i})\=\text{arg} \min\limits{\theta}-\sum{i=1}^{m}\log(p(y^{(i)}|x^{(i)},\theta))-\sum{i=1}^{m}\log(\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(\theta{i}-\mu)^{2}}{2\sigma^{2}}})\=\text{arg} \min\limits{\theta}-\sum{i=1}^{m}\log(p(y^{(i)}|x^{(i)},\theta))+\sum{i=1}^{m}\lambda(\theta_{i}-\mu)^{2}+\text{常量}$$L2正则又称为岭回归(Ridge Regression)，会导致参数在零附近很密集，但并不会出现很多零。 从下右图可以看出，交点并不为零。 L1参数正则化L1参数正则化相当于参数$\theta$的先验概率满足拉普拉斯分布，$p(\theta{i})=\frac{1}{2b}e^{-\frac{|\theta{i}-\mu|}{b}}$$$\theta{\text{MAP}}=\text{arg} \min\limits{\theta}-\sum{i=1}^{m}\log(p(y^{(i)}|x^{(i)},\theta))-\sum{i=1}^{m}\log p(\theta{i})\=\text{arg} \min\limits{\theta}-\sum{i=1}^{m}\log(p(y^{(i)}|x^{(i)},\theta))-\sum{i=1}^{m}\log \frac{1}{2b}e^{-\frac{|\theta{i}-\mu|}{b}}\=\text{arg} \min\limits{\theta}-\sum{i=1}^{m}\log(p(y^{(i)}|x^{(i)},\theta))+\sum{i=1}^{m}\lambda|\theta_{i}-\mu|+\text{常量}$$L1正则又称为Lasso，会导致参数为零，产生稀疏矩阵。 从下右图可以看出，交点为零。 数据增强(Data Augmentation)通过图像的几何变换, 使用一种或多种组合数据增强变换来增加输入数据的量，特别适用于对象识别这一任务 旋转|反射变换(Rotation/reflection): 随机旋转图像一定角度; 改变图像内容的朝向； 翻转变换(flip): 沿着水平或者垂直方向翻转图像； 缩放变换(zoom): 按照一定的比例放大或者缩小图像； 平移变换(shift): 在图像平面上对图像以一定方式进行平移 需要注意的是，在改变图像时，不要改变图像的类别，例如，图片‘6’与‘9’。 早期停止(early stopping)在每一个epoch结束时计算验证集的准确率，当准确率不再提高时，就停止训练。可以通过学习曲线来判断。 具体做法是，在训练的过程中，记录最佳的验证集准确率，当连续10次epoch（或者更多次）没达到最佳accuracy时，你可以认为“不再提高”，此时使用early stopping。这个策略就叫“ no-improvement-in-n”，n即epoch的次数，可以根据实际情况取10、20、30等。 dropout正则化(dropout regularization)大规模的神经网络有两个缺点：费时和过拟合，Dropout的出现很好地解决了这个问题。 在神经网络的训练过程中，按照一定的概率，将部分神经元暂时关闭。经过交叉验证发现，dropout率等于0.5的时候效果最好，此时网络结构最多。 因而，对于一个有n个节点的神经网络，有了dropout后，就可以看做是$2^{n}$个模型的集合了，但此时要训练的参数数目却是不变的，相当于Bagging。 参考 《Deeping Learning》]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>正则化</tag>
        <tag>early stopping</tag>
        <tag>数据增强</tag>
        <tag>dropout</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[反向传播神经网络(back propagation)]]></title>
    <url>%2F2017%2F10%2F14%2F%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C(Back%20propagation)%2F</url>
    <content type="text"><![CDATA[BP神经网络是一种多层的前馈神经网络(feedward neural network)，通过误差逆向传播算法来不断调整网络参数，使网络的误差达到最小，是目前应用最广泛的神经网络。 BP神经网络模型神经网络由很多层构成，每一层又由许多单元组成。神经网络第一层叫输入层，最后一层叫输出层，中间的各层叫隐藏层。/神经网络-8bc3c.png)训练一个BP神经网络，实质上就是调整网络的权重（W）和偏置（b）这两个参数。BP神经网络的训练过程分为两部分 前向传输，逐层波浪式的传递输出值。 逆向反馈，反向逐层调整权重和偏置。 前向传播算法单个样本(非向量化)$$z^{[1]}=W^{[1]}x + b^{[1]}\a^{[1]}=\sigma^{[1]}(z^{[1]})\z^{[2]}=W^{[2]}a^{[1]} + b^{[2]}\a^{[2]}=\sigma^{[2]}(z^{[2]})=\frac{1}{1+e^{-z[2]}} \text{输出层激活函数采用sigmoid}\\text{Loss Function} : \mathcal{L}(a^{[2]},y)=-y\log(a^{[2]})-(1-y)\log(1-a^{[2]})$$多个样本(向量化)$$Z^{[1]}=W^{[1]}X + b^{[1]}\A^{[1]}=\sigma^{[1]}(Z^{[1]})\Z^{[2]}=W^{[2]}A^{[1]} + b^{[2]}\A^{[2]}=\sigma^{[2]}(Z^{[2]})=\frac{1}{1+e^{-Z[2]}} \text{输出层激活函数采用sigmoid}\\text{Loss Function} : \mathcal{L}(A^{[2]},Y)=-Y\log(A^{[2]})-(1-Y)\log(1-A^{[2]})$$ 反向传播算法单个样本(非向量化)$$\partial\frac{\mathcal{L}}{a^{[2]}}=-\frac{y}{a^{[2]}}+\frac{1-y}{1-a^{[2]}}\\partial\frac{a^{[2]}}{z^{[2]}}=\frac{e^{-z^{[2]}}}{(1+e^{-z^{[2]}})^2}=(1-a^{[2]})\cdot a^{[2]}\\partial\frac{\mathcal{L}}{z^{[2]}}=\partial\frac{\mathcal{L}}{a^{[2]}}\cdot\partial\frac{\mathcal{a^{[2]}}}{z^{[2]}}=a^{[2]}-y\\partial\frac{\mathcal{L}}{W^{[2]}}=\partial\frac{\mathcal{L}}{z^{[2]}}\cdot\partial\frac{z^{[2]}}{W^{[2]}}=\partial\frac{\mathcal{L}}{z^{[2]}}\cdot a^{[1]T}\\partial\frac{\mathcal{L}}{b^{[2]}}=\partial\frac{\mathcal{L}}{z^{[2]}}\cdot\partial\frac{z^{[2]}}{b^{[2]}}=\partial\frac{\mathcal{L}}{z^{[2]}}\\partial\frac{\mathcal{L}}{a^{[1]}}=W^{[2]T}\cdot\partial\frac{\mathcal{L}}{z^{[2]}}\\partial\frac{\mathcal{L}}{z^{[1]}}=\partial\frac{\mathcal{L}}{a^{[1]}}\cdot\partial\frac{a^{[1]}}{z^{[1]}}=W^{[2]T}\cdot\partial\frac{\mathcal{L}}{z^{[2]}}\cdot\sigma^{[1]’}(z^{[1]})\\partial\frac{\mathcal{L}}{W^{[1]}}=\partial\frac{\mathcal{L}}{z^{[1]}}\cdot\partial\frac{z^{[1]}}{W^{[1]}}=\partial\frac{\mathcal{L}}{z^{[1]}}\cdot x^{T}\\partial\frac{\mathcal{L}}{b^{[1]}}=\partial\frac{\mathcal{L}}{z^{[1]}}\cdot\partial\frac{z^{[1]}}{b^{[1]}}=\partial\frac{\mathcal{L}}{z^{[1]}}\$$多个样本(向量化)$$\partial\frac{\mathcal{L}}{A^{[2]}} = -\frac{Y}{A^{[2]}} + \frac{1-Y}{1-A^{[2]}}\\partial\frac{A^{[2]}}{Z^{[2]}}=\frac{e^{-Z^{[2]}}}{(1+e^{-Z^{[2]}})^2}=(1-A^{[2]})\cdot Z^{[2]}\\partial\frac{\mathcal{L}}{Z^{[2]}}=\partial\frac{\mathcal{L}}{A^{[2]}}\cdot\partial\frac{\mathcal{Z^{[2]}}}{Z^{[2]}}=A^{[2]}-Y\\partial\frac{\mathcal{L}}{W^{[2]}}=\partial\frac{\mathcal{L}}{Z^{[2]}}\cdot\partial\frac{Z^{[2]}}{W^{[2]}}=\frac{1}{m}\partial\frac{\mathcal{L}}{z^{[2]}}\cdot a^{[1]T}\\partial\frac{\mathcal{L}}{b^{[2]}}=\partial\frac{\mathcal{L}}{Z^{[2]}}\cdot\partial\frac{Z^{[2]}}{b^{[2]}}=\frac{1}{m}np.sum(\partial\frac{\mathcal{L}}{Z^{[2]}},axis=1,keepdims=True)\\partial\frac{\mathcal{L}}{A^{[1]}}=W^{[2]T}\cdot\partial\frac{\mathcal{L}}{Z^{[2]}}\\partial\frac{\mathcal{L}}{Z^{[1]}}=\partial\frac{\mathcal{L}}{A^{[1]}}\cdot\partial\frac{A^{[1]}}{Z^{[1]}}=W^{[2]T}\cdot\partial\frac{\mathcal{L}}{Z^{[2]}}\cdot\sigma^{[1]’}(Z^{[1]})\\partial\frac{\mathcal{L}}{W^{[1]}}=\partial\frac{\mathcal{L}}{Z^{[1]}}\cdot\partial\frac{Z^{[1]}}{W^{[1]}}=\frac{1}{m}\partial\frac{\mathcal{L}}{Z^{[1]}}\cdot X^{T}\\partial\frac{\mathcal{L}}{b^{[1]}}=\partial\frac{\mathcal{L}}{Z^{[1]}}\cdot\partial\frac{Z^{[1]}}{b^{[1]}}=\frac{1}{m}np.sum(\partial\frac{\mathcal{L}}{Z^{[1]}},axis=1,keepdims=True)\$$ 搭建神经网络 特征归一化(normalization)控制特征值区间，使之与数据集中其他特征的区间匹配，可加快训练速度。 线性归一化(Min-Max scaling)。将原始数据线性转换到[0,1]区间内，适用于灰度转换等不服从正态分布的数据。$X{norm}=\frac{X-X{min}}{X{max}-X{min}}$ 0均值标准化(Z-score standardization)。将原始数据归一化为0均值，方差为1的数据集，适用于聚类、分类算法中服从正态分布的数据。$z=\frac{x-\mu}{\sigma}$ 网络的层数理论已经证明，一个S型隐藏层加上一个线性输出层，能够逼近任何有理函数，增加层数可以进一步降低误差，提高精度，但同时也使网络复杂化。 隐层神经元的个数增加神经元个数在结构实现上要比增加网络层数简单得多。 神经元数太少时，网络学习能力低，训练迭代的次数也比较多，训练精度也不高。 神经元数太多时，网络学习能力高，精确度也更高，训练迭代的次数也大，可能会出现过拟合(over fitting)现象。 因此，隐层神经元个数的选取原则是：在能够解决问题的前提下，增加一两个神经元，以加快误差下降速度。 参数初始化 全零初始化。导致网络中每个神经元都得到同样的输出，在反向传播中计算出同样的梯度，从而进行同样的参数更新。神经元之间就失去了不对称性的源头。 随机初始化。将权重初始化为很小的数值，以此来打破对称性。 12# n1为前一层神经元数，n2为本层神经元数w = np.random.randn(n1,n2) * 0.01 使用校准方差。随着输入数据量的增长，随机初始化的神经元的输出数据的分布中的方差也在增大。通过除以输入数据量的平方根来调整其数值范围，将神经元输出的方差就归一化。 1234# n1为前一层神经元数，n2为本层神经元数w = np.random.randn(n1,n2) * sqrt(2/n1)#ReLU神经元w = np.random.randn(n1,n2) * sqrt(2/n1)#tanh神经元w = np.random.randn(n1,n2) * sqrt(2/(n1 + n2))#最新的做法 学习速率 固定的学习速率。如果学习速率太小，则会使收敛过慢，如果学习速率太大，则会导致代价函数振荡。 学习率衰减。对于复杂的网络，在不同的阶段设置不同大小的学习速率，效果会更好。 ① $\alpha=\frac{1}{1+decay^{epoch}} \alpha_{0}$ ② $\alpha=0.95^{epoch} \alpha_{0}$ BP神经网络的缺陷 陷入局部极值，使训练失败。BP算法为一种局部搜索的优化方法，但它要解决的问题为求解复杂非线性函数的全局极值。 训练次数多使得学习效率低，收敛速度慢。 训练时学习新样本有遗忘旧样本的趋势。 参考：http://blog.csdn.net/lujiandong1/article/details/53320174http://blog.csdn.net/hjimce/article/details/50866313]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>BP神经网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习经典面试问题]]></title>
    <url>%2F2017%2F10%2F01%2F2017-10-1-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%BB%8F%E5%85%B8%E9%9D%A2%E8%AF%95%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[宏观问题 看过哪些资料书，有什么想法 做过的项目，讲一讲 觉得自己突出的地方 基础问题 训练集，验证集，测试集的划分和作用 损失函数的种类，及其优化方法 BGD SGD Adam 过拟合与欠拟合 L1、L2：优化方法，坐标下降，LARS角回归 项目 特征相关性 数据如何进行预处理 处理数据缺失值 特征提取方法，判断特征是否重要 如何调节模型参数 聚类算法，无法度量，kd树 BP神经网络 CNN RNN，长时依赖问题及解决 决策树C4.5,ID3，防止过拟合 SVM原问题与对偶问题，kkt,过拟合 LR 各种算法应用场景 算法与数据结构 链表逆序 有序数组的交集 随机数1-5如何输出1-7 链表有环 连续乘机，溢出 面试注意要点 主动输出]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>面试</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习中的偏差与方差]]></title>
    <url>%2F2017%2F09%2F27%2F%E5%81%8F%E5%B7%AE%E4%B8%8E%E6%96%B9%E5%B7%AE%2F</url>
    <content type="text"><![CDATA[泛化误差模型的泛化能力（generalization ability）是指模型对未知数据的预测能力。同理，泛化误差（generalization error）是指模型对未知数据预测的误差。 泛化误差可分为：随机误差（random error）、偏差（bias）和方差（variance）。其中，随机误差是数据本身的噪声带来的，无法避免。 本文主要讲偏差和方差。 偏差与方差偏差表示参数与真实参数之间的误差。$$bias(\hat{\theta}{m})=\mathcal{E}(\hat{\theta}{m})-\theta$$当模型做出与实际情况不符的假设时就会引起错误（通常是模型太简单），就会导致偏差大，即对数据欠拟合。 方差表示模型在拟合不同的数据时，参数的变化程度。$$Var(\hat{\theta})$$在理想情况下，同一来源的数据应满足独立同分布。那么针对同一问题，无论使用哪个数据集，所建立的模型应当是相同的。从公式的角度来说，就是目标方程的系数和常数项应当是近似相同的。但如果模型太复杂，那么随着训练数据集的不同，参数变化太大时，就会导致方差大，即对数据过拟合。 下图能够很好地解释偏差和方差。 在一个实际系统中，偏差与方差往往不能兼得。如果注重模型在训练数据上的准确度，忽视对模型的先验知识，这样可以减少模型的偏差，但会降低模型的泛化能力，造成方差增大。如果注重对模型的先验知识，对模型增加更多的限制，这样可以降低模型的方差，但也会降低模型在训练数据上的准确度，造成偏差增大。 Trade-Off方差与偏差之间的trade-off是机器学习的重要主题。 模型误差是偏差与方差之和，因此可以绘制下图。 从图中可以看出 当模型太简单时，拟合能力不够强，偏差比较大；由于拟合能力不强, 数据集的扰动也无法使模型产生显著变化，方差较小。模型欠拟合。 随着模型复杂度提升，拟合能力逐渐增强，预测渐渐准确，偏差持续减小；数据的扰动也能够渐渐被学习器学到，方差增大。模型较优。 当模型太复杂时，拟合能力非常强，偏差很小；数据的轻微扰动都会导致模型发生显著变化，方差很大。此时，训练数据自身的、非全局的特性被模型捕捉到，模型过拟合。 Total Error曲线的拐点，是模型的最优位置。 将数据分为训练集和验证集，可以绘制出下图。 从图中可以看出 当模型太简单时，对训练集和验证集的预测误差较大，模型高偏差，欠拟合。 随着模型复杂度提升，拟合能力增强，对训练集和验证集的预测误差都减小，模型较优。 当模型太复杂时，对训练集过分拟合，训练误差进一步减小。然而泛化能力不高，验证集误差较大，模型高方差，过拟合。 如何处理欠拟合和过拟合参考吴恩达教授提供的处理模型欠拟合与过拟合的一般方法。 当模型处于欠拟合状态时，应该增加模型的复杂度。一般办法有： 增加模型迭代次数。 提高模型复杂度：例如在神经网络中增加神经网络层数、在SVM中用非线性SVM（核技术）代替线性SVM。 增加特征：特征少，对模型信息的刻画就不足够了 降低正则化权重：正则化为了限制模型的复杂度而设定的，降低其权值可以在模型训练中增加模型复杂度。 当模型处于过拟合状态时，应该降低模型的复杂度。一般方法有： 增加数据： 减少特征：PCA 正则化： 参考：http://scott.fortmann-roe.com/docs/BiasVariance.htmlhttps://plushunter.github.io/]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>方差</tag>
        <tag>偏差</tag>
        <tag>过拟合</tag>
        <tag>欠拟合</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[激活函数(Activation Function)]]></title>
    <url>%2F2017%2F09%2F26%2F%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0(Activation%20Function)%2F</url>
    <content type="text"><![CDATA[在机器学习中，尤其是神经网络中，会经常用到激活函数。本篇总结常见的激活函数及其优缺点。 激活函数的性质: 非线性：激活函数一般是非线性的，用来给网络加入非线性因素，增强网络的表达能力。如果使用线性激活函数，那么线性的组合仍然是线性。即，无论网络中有多少层神经元，其结果都相当于一层神经元。 可微性：当优化方法是基于梯度的时候，这个性质是必须的。 常见的激活函数常见的激活函数有：sigmoid函数、tanh函数、ReLU函数、Leaky-ReLU函数 sigmoid函数数学形式$$sigmoid(x) = \frac{1}{1+e^{-x}}$$形状 /markdown-img-paste-20170926080536411.png) 描述 结果为[0,1]，均值为0.5。 优点 结果为[0,1]。在二元分类中，常用来做输出层的激活函数。 缺点 软饱和。当输入极大或者极小时，神经元的梯度是接近于0，处于饱和状态，网络变得很难学习。 偏移现象。均值为0.5，下一层神经元的输入均值不为0。 tanh函数数学形式$$tanh(x)=2sigmoid(2x)-1=\frac{2}{1+e^{-2x}}-1=\frac{e^{x}-e^{-x}}{e^{x}+e^{-x}}$$形状 /markdown-img-paste-20170926080437430.png) 描述 结果为[-1,1]，均值为0。 优点 均值为0。不存在偏移现象。 缺点 软饱和。当输入极大或者极小时，神经元的梯度是接近于0，处于饱和状态，网络变得很难学习。 ReLU函数数学形式$$ReLU(x) = max(0,x)$$形状 /markdown-img-paste-20170926091252946.png) 描述 结果为[0,∞]。 优点 计算速度快。ReLU只需要一个阈值就可以得到激活值，计算简单。 收敛速度快。在大于0的区域其梯度恒为1，不存在饱和现象。 缺点 偏移现象。均值不为0。 硬饱和。在小于0的区域其梯度为0。 神经元死亡。当输入落入小于0的区域时，其梯度等于0，导致对应权重无法更新。减小学习速率可以缓解。 Leaky-ReLU函数数学形式$$Leaky-ReLU(x) = max(\alpha x,x),\alpha = 0.01$$形状 /markdown-img-paste-20170926091702440.png) 描述 结果为[-∞,∞]。 优点 计算速度快。ReLU只需要一个阈值就可以得到激活值，计算简单。 收敛速度快。在大于0的区域其梯度恒为1，不存在饱和现象。 解决了神经元死亡问题。在小于0的区域，存在梯度不为0。 缺点 不够成熟，不常用。 如何选择 sigmoid函数常用于二元分类神经网络的输出层。 ReLU函数常用于神经网络的隐藏层，使用时应注意将learning-rate设置小一点，避免神经元死亡。 硬饱和与软饱和 硬饱和。函数的导数等于0 软饱和。函数的导数趋近于0]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>激活函数</tag>
        <tag>Sigmoid</tag>
        <tag>Tanh</tag>
        <tag>ReLU</tag>
      </tags>
  </entry>
</search>
