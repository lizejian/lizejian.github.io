<!DOCTYPE html>



  


<html class="theme-next mist use-motion" lang="zh-Hans">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.2" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="SGD,Momentum,Nesterov动量,AdaGrad,RMSProp,Adam,Batch Normlization," />








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.2" />






<meta name="description" content="神经网络优化中的挑战局部极小值是否存在大量偏差严重的局部极小值，优化算法是否会碰到这些局部极小值，都是尚未解决的公开问题。 悬崖与梯度爆炸多层神经网络通常有像悬崖一样的斜率较大区域，这是由于几个较大的权重相乘导致的。遇到斜率极大的悬崖结构时，梯度更新会很大程度地改变参数值，通常会跳过这类悬崖结构。  不管我们是从上还是从下接近悬崖，情况都很糟糕，但幸运的是可以用使用启发式梯度截断 (gradien">
<meta name="keywords" content="SGD,Momentum,Nesterov动量,AdaGrad,RMSProp,Adam,Batch Normlization">
<meta property="og:type" content="article">
<meta property="og:title" content="深度模型中的优化">
<meta property="og:url" content="http://lizejian.github.io.com/2017/10/17/深度模型中的优化/index.html">
<meta property="og:site_name" content="Zejian&#39;s Blog">
<meta property="og:description" content="神经网络优化中的挑战局部极小值是否存在大量偏差严重的局部极小值，优化算法是否会碰到这些局部极小值，都是尚未解决的公开问题。 悬崖与梯度爆炸多层神经网络通常有像悬崖一样的斜率较大区域，这是由于几个较大的权重相乘导致的。遇到斜率极大的悬崖结构时，梯度更新会很大程度地改变参数值，通常会跳过这类悬崖结构。  不管我们是从上还是从下接近悬崖，情况都很糟糕，但幸运的是可以用使用启发式梯度截断 (gradien">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="http://lizejian.github.io.com/2017/10/17/深度模型中的优化/2017-10-17-深度模型中的优化/梯度悬崖.png">
<meta property="og:image" content="http://lizejian.github.io.com/2017/10/17/深度模型中的优化/2017-10-17-深度模型中的优化/不带动量的SGD.png">
<meta property="og:image" content="http://lizejian.github.io.com/2017/10/17/深度模型中的优化/2017-10-17-深度模型中的优化/带动量的SGD.png">
<meta property="og:image" content="http://lizejian.github.io.com/2017/10/17/深度模型中的优化/2017-10-17-深度模型中的优化/nesterov动量.png">
<meta property="og:image" content="http://lizejian.github.io.com/2017/10/17/深度模型中的优化/2017-10-17-深度模型中的优化/AdaGrad.png">
<meta property="og:image" content="http://lizejian.github.io.com/2017/10/17/深度模型中的优化/2017-10-17-深度模型中的优化/RMSprop.png">
<meta property="og:image" content="http://lizejian.github.io.com/2017/10/17/深度模型中的优化/2017-10-17-深度模型中的优化/Nesterov-RMSProp.png">
<meta property="og:image" content="http://lizejian.github.io.com/2017/10/17/深度模型中的优化/2017-10-17-深度模型中的优化/Adam.png">
<meta property="og:image" content="http://img.blog.csdn.net/20160223160053859">
<meta property="og:image" content="http://img.blog.csdn.net/20160223160132599">
<meta property="og:updated_time" content="2017-11-01T08:56:31.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="深度模型中的优化">
<meta name="twitter:description" content="神经网络优化中的挑战局部极小值是否存在大量偏差严重的局部极小值，优化算法是否会碰到这些局部极小值，都是尚未解决的公开问题。 悬崖与梯度爆炸多层神经网络通常有像悬崖一样的斜率较大区域，这是由于几个较大的权重相乘导致的。遇到斜率极大的悬崖结构时，梯度更新会很大程度地改变参数值，通常会跳过这类悬崖结构。  不管我们是从上还是从下接近悬崖，情况都很糟糕，但幸运的是可以用使用启发式梯度截断 (gradien">
<meta name="twitter:image" content="http://lizejian.github.io.com/2017/10/17/深度模型中的优化/2017-10-17-深度模型中的优化/梯度悬崖.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    sidebar: {"position":"left","display":"post","offset":12,"offset_float":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://lizejian.github.io.com/2017/10/17/深度模型中的优化/"/>





  <title>深度模型中的优化 | Zejian's Blog</title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  














</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left page-post-detail ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Zejian's Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">这么好的故事，你可别演砸了。</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://lizejian.github.io.com/2017/10/17/深度模型中的优化/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Zejian">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zejian's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">深度模型中的优化</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-10-17T09:48:29+08:00">
                2017-10-17
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/深度学习/" itemprop="url" rel="index">
                    <span itemprop="name">深度学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          
            <span class="post-meta-divider">|</span>
            <span class="page-pv"><i class="fa fa-file-o"></i>
            <span class="busuanzi-value" id="busuanzi_value_page_pv" ></span>次
            </span>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h1 id="神经网络优化中的挑战"><a href="#神经网络优化中的挑战" class="headerlink" title="神经网络优化中的挑战"></a>神经网络优化中的挑战</h1><h2 id="局部极小值"><a href="#局部极小值" class="headerlink" title="局部极小值"></a>局部极小值</h2><p>是否存在大量偏差严重的局部极小值，优化算法是否会碰到这些局部极小值，都是尚未解决的公开问题。</p>
<h2 id="悬崖与梯度爆炸"><a href="#悬崖与梯度爆炸" class="headerlink" title="悬崖与梯度爆炸"></a>悬崖与梯度爆炸</h2><p>多层神经网络通常有像悬崖一样的斜率较大区域，这是由于几个较大的权重相乘导致的。遇到斜率极大的悬崖结构时，梯度更新会很大程度地改变参数值，通常会跳过这类悬崖结构。</p>
<p><img src="2017-10-17-深度模型中的优化/梯度悬崖.png" alt=""></p>
<p>不管我们是从上还是从下接近悬崖，情况都很糟糕，但幸运的是可以用使用<strong>启发式梯度截断</strong> (gradient clipping) 来避免其主要缺点。基本想法源自考虑到梯度并没有指明最佳步长，只是说明在无限小区域内的最佳方向。当传统的梯度下降算法提议更新很大一步时，启发式梯度截断会干涉去减小步长，从而使其不太可能走出梯度很大的悬崖区域。悬崖结构在循环神经网络的损失函数中很常见，因为这类模型会涉及到多个时间步长因素的相乘。</p>
<h2 id="长期依赖"><a href="#长期依赖" class="headerlink" title="长期依赖"></a>长期依赖</h2><p>当计算图变得极深时，神经网络优化算法会面临的另外一个难题就是长期依赖问题——由于变深的结构使模型丧失了学习到先前信息的能力，让优化变得极其困难。 </p>
<h2 id="梯度消失与梯度爆炸"><a href="#梯度消失与梯度爆炸" class="headerlink" title="梯度消失与梯度爆炸"></a>梯度消失与梯度爆炸</h2><p>梯度消失与梯度爆炸是指该计算图上的梯度也会因为网络过深而大幅度变化。消失的梯度使得难以知道参数朝哪个方向移动能够改进损失函数，而爆炸梯度会使得学习不稳定。悬崖结构便是爆炸梯度现象的一个例子。    </p>
<p>梯度不稳定的原因在于网络本身，在计算每层梯度时会涉及连乘操作。当网络过深时，如果连乘的因子小于1，乘积可能趋于0；如果连乘的因子大于1，乘积可能趋于无穷。<br>解决梯度消失问题：不使用sigmoid函数。sigmoid函数的最大梯度仅为1/4，连乘很容易造成梯度消失。<br>解决梯度爆炸问题：①学习率衰减。在每一层使用不同的学习速率，使每一层的乘积可以维持稳定。②<strong>梯度截断</strong>。在应用梯度之前先修饰数值，有助于确保数值稳定性，防止梯度爆炸出现。        </p>
<h1 id="基本算法"><a href="#基本算法" class="headerlink" title="基本算法"></a>基本算法</h1><h2 id="批量梯度下降-Batch-Gradient-Descend"><a href="#批量梯度下降-Batch-Gradient-Descend" class="headerlink" title="批量梯度下降(Batch Gradient Descend)"></a>批量梯度下降(Batch Gradient Descend)</h2><p>批量梯度下降(BGD)在计算参数时，使用整个训练集，沿着整个训练集的梯度方向下降。<br>$$<br>\theta<em>{\text{ML}}=\arg\max\limits</em>{\theta}\sum_{i=1}^{m}\log p(x^{(i)},y^{(i)};\theta)<br>$$<br>当数据量太大时，计算速度会很慢。BGD使用固定的学习速率。</p>
<h2 id="随机梯度下降-Stochastic-Gradient-Descend"><a href="#随机梯度下降-Stochastic-Gradient-Descend" class="headerlink" title="随机梯度下降(Stochastic Gradient Descend)"></a>随机梯度下降(Stochastic Gradient Descend)</h2><p>随机梯度下降(SGD)按照数据生成分布抽取m个小批量(独立同分布)的mini-batch样本，沿着随机挑选的mini-batch数据的梯度方向下降。</p>
<p>SGD算法中的一个关键参数是学习速率。在实践中，有必要随着时间的推移逐渐降低学习速率，我们将第 k 步<br>迭代的学习速率记作 $\epsilon_{k}$。</p>
<p>实践中，一般会线性衰减学习率直到第$\tau$次迭代:<br>$$<br>\epsilon=(1-\alpha)\epsilon<em>{0}+\alpha\epsilon</em>{\tau},\alpha=\frac{k}{\tau}<br>$$<br>在$\tau$步迭代之后，一般使$\tau$保持常数。参数选择为 $\epsilon<em>{0}$，$\epsilon</em>{\tau}$，$\tau$。通常$\tau$被设为需要反复遍历训练样本几百次的迭代次数。通常$\epsilon<em>{\tau}$应设为大于1% 的$\epsilon</em>{0}$。主要问题是如何设置$\epsilon<em>{0}$。若$\epsilon</em>{0}$太大，学习曲线将会剧烈振荡，损失函数值通常会明显增加。温和的振荡是良好的，特别是训练于随机损失函数上。如果学习速率太慢，那么学习进程会缓慢。如果初始学习速率太低，那么学习可能会卡在一个相当高的损失值。通常，就总训练时间和最终损失值而言，最优初始学习速率会高于大约迭代 100 步后输出最好效果的学习速率。因此，通常最好是检测最早的几轮迭代，使用一个高于此时效果最佳学习速率的学习速率，但又不能太高以致严重的不稳定性。</p>
<h2 id="动量-Momentum"><a href="#动量-Momentum" class="headerlink" title="动量(Momentum)"></a>动量(Momentum)</h2><p>随机梯度下降(SGD)方法中的高方差振荡使得网络很难稳定收敛，所以有研究者提出了一种称为动量的技术，通过优化相关方向的训练和弱化无关方向的振荡，来加速SGD训练。</p>
<p>动量算法引入了变量$v$充当速度的角色——它代表参数在参数空间移动的方向和速度。</p>
<p>更新规则如下<br>$$<br>v\gets\alpha v-\epsilon\nabla<em>{\theta}\lgroup\frac{1}{m}\sum</em>{i=1}^{m}\mathcal{L}(f(x^{(i)};\theta),y^{(i)})\rgroup\<br>\theta\gets\theta+v<br>$$<br>速度$v​$累积了梯度元素$\nabla<em>{\theta}\lgroup\frac{1}{m}\sum</em>{i=1}^{m}\mathcal{L}(f(x^{(i)};\theta),y^{(i)})\rgroup​$。$\alpha​$ 越大，之前梯度对现在方向的影响也越大，$\alpha​$通常设定为0.9。在参数更新过程中，使网络能更优和更稳定的收敛， 减少振荡过程。</p>
<p>不带动量的SGD与带动量的SGD比较如下：</p>
<p><img src="2017-10-17-深度模型中的优化/不带动量的SGD.png" alt=""> <img src="2017-10-17-深度模型中的优化/带动量的SGD.png" alt=""></p>
<p>一个条件数较差的二次目标函数看起来像一个长而窄的山谷或陡峭的峡谷。可以看出，带动量的梯度下降能够正确地纵向穿过峡谷，而不带动量的梯度下降则会浪费时间在峡谷的窄轴上来回移动。</p>
<h2 id="Nesterov-动量"><a href="#Nesterov-动量" class="headerlink" title="Nesterov 动量"></a>Nesterov 动量</h2><p>Nesterov 动量可以解释为在标准动量方法中添加了一个校正因子。<br>$$<br>v\gets\alpha v\gets\epsilon\nabla<em>{\theta}\lgroup\frac{1}{m}\sum</em>{i=1}^{m}\mathcal{L}(f(x^{(i)};\theta+\alpha v),y^{(i)})\rgroup\<br>\theta\gets\theta+v<br>$$<br><img src="2017-10-17-深度模型中的优化/nesterov动量.png" alt=""></p>
<h1 id="自适应学习率算法"><a href="#自适应学习率算法" class="headerlink" title="自适应学习率算法"></a>自适应学习率算法</h1><h2 id="AdaGrad"><a href="#AdaGrad" class="headerlink" title="AdaGrad"></a>AdaGrad</h2><p>AdaGrad算法能够独立地适应所有模型参数的学习速率，放缩每个参数反比于其所有梯度历史平方值总和的平方根。具有损失最大偏导的参数相应地有一个快速下降的学习速率，而具有小偏导的参数在学习速率上有相对较小的下降。净效果是在参数空间中更为平缓的倾斜方向会取得更大的进步。</p>
<p><img src="2017-10-17-深度模型中的优化/AdaGrad.png" alt=""></p>
<p>AdaGrad算法具有一些令人满意的理论性质。然而，经验上已经发现，对于训练深度神经网络模型而言，从训练开始时积累梯度平方会导致有效学习速率过早和过量的减小。AdaGrad 在某些深度学习模型上效果不错，但不是<br>全部。</p>
<h2 id="RMSProp"><a href="#RMSProp" class="headerlink" title="RMSProp"></a>RMSProp</h2><p>RMSProp算法修改 AdaGrad 以在非凸设定下效果更好，改变梯度积累为指数加权的移动均值。AdaGrad 根据平方梯度的整个历史收缩学习率，可能使得学习率在达到这样的凸结构前就变得太小了。RMSProp 使用<strong>指数衰减平均</strong>以丢弃遥远过去的历史，使其能够在找到凸碗状结构后快速收敛，它就像一个初始化于该碗状结构的 AdaGrad 算法实例。</p>
<p><img src="2017-10-17-深度模型中的优化/RMSprop.png" alt=""></p>
<p>​                  <img src="2017-10-17-深度模型中的优化/Nesterov-RMSProp.png" alt="">    </p>
<h2 id="Adam"><a href="#Adam" class="headerlink" title="Adam"></a>Adam</h2><p>Adam将动量直接并入了梯度一阶矩（指数加权）的估计，其次，Adam包括偏置修正，修正从原点初始化的一阶矩（动量项）和（非中心的）二阶矩的估计。Adam通常被认为对超参数的选择相当鲁棒，尽管学习率有时需要从建议的默认修改。</p>
<p><img src="2017-10-17-深度模型中的优化/Adam.png" alt=""></p>
<p>在实际应用中，Adam方法效果良好。与其他自适应学习率算法相比，其收敛速度更快，学习效果更为有效，而且可以纠正其他优化技术中存在的问题，如学习率消失、收敛过慢或是高方差的参数更新导致损失函数波动较大等问题。</p>
<h2 id="选择正确的优化方法"><a href="#选择正确的优化方法" class="headerlink" title="选择正确的优化方法"></a>选择正确的优化方法</h2><p>Adam在实际应用中效果良好，超过了其他的自适应技术。</p>
<p>如果输入数据集比较稀疏，SGD和动量项等方法可能效果不好。因此对于稀疏数据集，应该使用某种自适应学习率的方法，且另一好处为不需要人为调整学习率，使用默认参数就可能获得最优值。</p>
<p>如果想使训练深层网络模型快速收敛或所构建的神经网络较为复杂，则应该使用Adam或其他自适应学习速率的方法，因为这些方法的实际效果更优。            </p>
<h1 id="优化策略"><a href="#优化策略" class="headerlink" title="优化策略"></a>优化策略</h1><h2 id="Batch-Normalization"><a href="#Batch-Normalization" class="headerlink" title="Batch Normalization"></a>Batch Normalization</h2><p>网络训练过程中参数不断改变导致后续每一层输入的分布也发生变化，而学习的过程又要使每一层适应输入的分布，因此我们不得不降低学习率、小心地初始化。</p>
<p>批归一化(BN)即将某一层输出归一化，使得其均值为0方差为1。值得注意的是，BN是在channel维度做的，即将每个channel都进行归一化，如果有n个channel那么便会有n个归一化操作。具体来说如果某个层的输出为$x=(x^{0},x^{1},…,x^{n})$，那么<br>$$<br>\hat{x}^{k}=\frac{x^{k}-E[x^{k}]}{\sqrt{Var[x^{k}]}}<br>$$<br>但如果简单地这么干，会降低层的表达能力。比如下图，在使用sigmoid激活函数的时候，如果把数据限制到0均值单位方差，那么相当于只使用了激活函数中近似线性的部分，这显然会降低模型表达能力。</p>
<p><img src="http://img.blog.csdn.net/20160223160053859" alt=""></p>
<p>为此，又为BN增加了2个参数，用来保持模型的表达能力。 于是最后的输出为<br>$$<br>y^{k}=\gamma^{k}\hat{x}+\beta^{k}<br>$$<br>上述公式中用到了均值E和方差Var，需要注意的是理想情况下E和Var应该是针对整个数据集的，但显然这是不现实的。因此，用一个Batch的均值和方差作为对整个数据集均值和方差的估计。</p>
<p><img src="http://img.blog.csdn.net/20160223160132599" alt=""></p>
<p>BN可以加快网络收敛速度，十分好用。</p>
<p>参考</p>
<p>《Deeping Learning》</p>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/SGD/" rel="tag"># SGD</a>
          
            <a href="/tags/Momentum/" rel="tag"># Momentum</a>
          
            <a href="/tags/Nesterov动量/" rel="tag"># Nesterov动量</a>
          
            <a href="/tags/AdaGrad/" rel="tag"># AdaGrad</a>
          
            <a href="/tags/RMSProp/" rel="tag"># RMSProp</a>
          
            <a href="/tags/Adam/" rel="tag"># Adam</a>
          
            <a href="/tags/Batch-Normlization/" rel="tag"># Batch Normlization</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2017/10/15/深度学习中的正则化/" rel="next" title="深度学习中的正则化">
                <i class="fa fa-chevron-left"></i> 深度学习中的正则化
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2017/10/19/卷积神经网络(Convolutional Neural Network)/" rel="prev" title="卷积神经网络(Convolutional Neural Network）">
                卷积神经网络(Convolutional Neural Network） <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          
  <div class="comments" id="comments">
    
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/images/avatar.jpg"
               alt="Zejian" />
          <p class="site-author-name" itemprop="name">Zejian</p>
           
              <p class="site-description motion-element" itemprop="description"></p>
          
        </div>
        <nav class="site-state motion-element">

          
            <div class="site-state-item site-state-posts">
              <a href="/archives/">
                <span class="site-state-item-count">9</span>
                <span class="site-state-item-name">日志</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-categories">
              <a href="/categories/index.html">
                <span class="site-state-item-count">3</span>
                <span class="site-state-item-name">分类</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-tags">
              <a href="/tags/index.html">
                <span class="site-state-item-count">25</span>
                <span class="site-state-item-name">标签</span>
              </a>
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
                <a href="https://github.com/lizejian" target="_blank" title="GitHub">
                  
                    <i class="fa fa-fw fa-github"></i>
                  
                    
                      GitHub
                    
                </a>
              </span>
            
          
        </div>

        
        

        
        

        


      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#神经网络优化中的挑战"><span class="nav-number">1.</span> <span class="nav-text">神经网络优化中的挑战</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#局部极小值"><span class="nav-number">1.1.</span> <span class="nav-text">局部极小值</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#悬崖与梯度爆炸"><span class="nav-number">1.2.</span> <span class="nav-text">悬崖与梯度爆炸</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#长期依赖"><span class="nav-number">1.3.</span> <span class="nav-text">长期依赖</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#梯度消失与梯度爆炸"><span class="nav-number">1.4.</span> <span class="nav-text">梯度消失与梯度爆炸</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#基本算法"><span class="nav-number">2.</span> <span class="nav-text">基本算法</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#批量梯度下降-Batch-Gradient-Descend"><span class="nav-number">2.1.</span> <span class="nav-text">批量梯度下降(Batch Gradient Descend)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#随机梯度下降-Stochastic-Gradient-Descend"><span class="nav-number">2.2.</span> <span class="nav-text">随机梯度下降(Stochastic Gradient Descend)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#动量-Momentum"><span class="nav-number">2.3.</span> <span class="nav-text">动量(Momentum)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Nesterov-动量"><span class="nav-number">2.4.</span> <span class="nav-text">Nesterov 动量</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#自适应学习率算法"><span class="nav-number">3.</span> <span class="nav-text">自适应学习率算法</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#AdaGrad"><span class="nav-number">3.1.</span> <span class="nav-text">AdaGrad</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#RMSProp"><span class="nav-number">3.2.</span> <span class="nav-text">RMSProp</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Adam"><span class="nav-number">3.3.</span> <span class="nav-text">Adam</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#选择正确的优化方法"><span class="nav-number">3.4.</span> <span class="nav-text">选择正确的优化方法</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#优化策略"><span class="nav-number">4.</span> <span class="nav-text">优化策略</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Batch-Normalization"><span class="nav-number">4.1.</span> <span class="nav-text">Batch Normalization</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2017</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Zejian</span>
</div>


<div class="powered-by">
  由 <a class="theme-link" href="https://hexo.io">Hexo</a> 强力驱动
</div>

<div class="theme-info">
  主题 -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Mist
  </a>
</div>


        
<div class="busuanzi-count">
  <script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="site-uv">
      <i class="fa fa-user"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
      人
    </span>
  

  
</div>


        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.2"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.2"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.2"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.2"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.2"></script>



  


  




	





  





  






  





  

  

  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->
  


  

  

</body>
</html>
