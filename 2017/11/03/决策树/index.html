<!DOCTYPE html>



  


<html class="theme-next mist use-motion" lang="zh-Hans">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.2" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="决策树," />








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.2" />






<meta name="description" content="决策树(decision tree)是一种基本的分类与回归方法。决策树模型是树形结构，在分类问题中，表示基于特征对实例进行分类的过程。它可以认为是if-then规则的集合，也可以认为是定义在特征空间与类空间上的条件概率分布。其主要优点是具有可读性，分类速度快。 决策树学习通常包括3个步骤：特征选择，决策树的生成和决策树的修剪。 决策树模型与学习决策树模型分类决策树模型是一种描述对实例进行分类的树形">
<meta name="keywords" content="决策树">
<meta property="og:type" content="article">
<meta property="og:title" content="决策树">
<meta property="og:url" content="http://lizejian.github.io.com/2017/11/03/决策树/index.html">
<meta property="og:site_name" content="Zejian&#39;s Blog">
<meta property="og:description" content="决策树(decision tree)是一种基本的分类与回归方法。决策树模型是树形结构，在分类问题中，表示基于特征对实例进行分类的过程。它可以认为是if-then规则的集合，也可以认为是定义在特征空间与类空间上的条件概率分布。其主要优点是具有可读性，分类速度快。 决策树学习通常包括3个步骤：特征选择，决策树的生成和决策树的修剪。 决策树模型与学习决策树模型分类决策树模型是一种描述对实例进行分类的树形">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/2717543-9809772d6687ccc4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://lizejian.github.io.com/2017/11/03/决策树/2017-11-03-决策树/屏幕快照%202017-11-03%20下午8.53.31.png">
<meta property="og:updated_time" content="2017-11-11T14:32:49.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="决策树">
<meta name="twitter:description" content="决策树(decision tree)是一种基本的分类与回归方法。决策树模型是树形结构，在分类问题中，表示基于特征对实例进行分类的过程。它可以认为是if-then规则的集合，也可以认为是定义在特征空间与类空间上的条件概率分布。其主要优点是具有可读性，分类速度快。 决策树学习通常包括3个步骤：特征选择，决策树的生成和决策树的修剪。 决策树模型与学习决策树模型分类决策树模型是一种描述对实例进行分类的树形">
<meta name="twitter:image" content="http://upload-images.jianshu.io/upload_images/2717543-9809772d6687ccc4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    sidebar: {"position":"left","display":"post","offset":12,"offset_float":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://lizejian.github.io.com/2017/11/03/决策树/"/>





  <title>决策树 | Zejian's Blog</title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  














</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left page-post-detail ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Zejian's Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">这么好的故事，你可别演砸了。</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://lizejian.github.io.com/2017/11/03/决策树/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Zejian">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zejian's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">决策树</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-11-03T19:40:14+08:00">
                2017-11-03
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/机器学习/" itemprop="url" rel="index">
                    <span itemprop="name">机器学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          
            <span class="post-meta-divider">|</span>
            <span class="page-pv"><i class="fa fa-file-o"></i>
            <span class="busuanzi-value" id="busuanzi_value_page_pv" ></span>次
            </span>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>决策树(decision tree)是一种基本的分类与回归方法。决策树模型是树形结构，在分类问题中，表示基于特征对实例进行分类的过程。它可以认为是<strong>if-then</strong>规则的集合，也可以认为是定义在特征空间与类空间上的条件概率分布。其主要优点是具有<strong>可读性</strong>，分类速度快。</p>
<p>决策树学习通常包括3个步骤：<strong>特征选择</strong>，<strong>决策树的生成</strong>和<strong>决策树的修剪</strong>。</p>
<h1 id="决策树模型与学习"><a href="#决策树模型与学习" class="headerlink" title="决策树模型与学习"></a>决策树模型与学习</h1><h2 id="决策树模型"><a href="#决策树模型" class="headerlink" title="决策树模型"></a>决策树模型</h2><p>分类决策树模型是一种描述对实例进行分类的树形结构，决策树由结点和有向边组成。结点有两种类型：内部结点与叶结点。内部结点表示一个特征或属性，叶结点表示一个类。</p>
<p>用决策树分类，从根结点开始，对实例的某一特征进行测试，根据测试结果，将实例分配到其子结点：这时，每一个子结点对应着该特征的一个取值。如此递归地对实例进行测试并分配，直至达到叶结点。最后将实例分到叶结点的类中。</p>
<p><img src="http://upload-images.jianshu.io/upload_images/2717543-9809772d6687ccc4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>可以将决策树看成一个if-then规则的集合。决策树对应的if-then规则集合具有一个重要的性质：<strong>互斥且完备</strong>。</p>
<h2 id="优点与缺点"><a href="#优点与缺点" class="headerlink" title="优点与缺点"></a>优点与缺点</h2><ol>
<li>决策树算法中学习简单的决策规则建立决策树模型的过程非常容易理解</li>
<li>决策树模型可以可视化，非常直观</li>
<li>应用范围广，可用于分类和回归，而且非常容易做多类别的分类</li>
<li>很容易在训练数据中生成复杂的树结构，造成过拟合（overfitting）。剪枝可以缓解过拟合的负作用，常用方法是限制树的高度、叶子节点中的最少样本数量。</li>
</ol>
<h2 id="决策树学习"><a href="#决策树学习" class="headerlink" title="决策树学习"></a>决策树学习</h2><p>决策树学习本质上是从训练数据集中归纳出一组分类规则。与训练数据集不相矛盾的决策树（即能贵训练数据进行正确分类的决策树）可能有多个，也可能一个没有。我们需要的是一个与训练数据矛盾较小的决策树，同时具有很好的泛化能力。从另一个角度看，决策树学习是由训练数据集估计<strong>条件概率模型</strong>。</p>
<p>决策树学习用损失函数表示这一目标。如下所述，决策树学习的损失函数通常是正则化的极大似然函数。</p>
<p><strong>决策树学习的算法通常是一个递归地选择最优特征，并根据该特征对训练数据进行分割，使得对各个子数据集有一个最好的分类的过程</strong>。这一过程对应着对特征空间的划分，也对应着决策树的构建。开始，构建根节点，将所有训练数据都放在根结点，选择一个最优特征，按照这一特征将训练数据分割成子集，使得各个子集有一个在当前条件下最好的分类。如果这些子集被基本正确分类，那么构建叶结点，将这些子集分到对应的叶结点中去；如果还有自己不能被正确分类，那就对这些子集选择新的最优特征，继续进行分类。如此递归下去，直到所有训练数据子集都被基本正确分类。</p>
<p>以上方法产生的决策树可能对训练数据有很好的拟合能力，但对未知的测试数据却未必有很好的分类能力，即可能发生过拟合现象。我们需要对已生成的树<strong>自下而上进行剪枝</strong>，将树变得简单，从而提升泛化能力。具体地，就是去掉过于细分的叶结点，使其会退到父结点，甚至更高的结点，然后将父结点或更高的结点改为新的叶结点。</p>
<p>如果特征数量过多，也可以决策树开始的时候，对特征进行选择，只留下对训练数据就有足够分类能力的特征。</p>
<h1 id="特征选择"><a href="#特征选择" class="headerlink" title="特征选择"></a>特征选择</h1><p>特征选择在于选取对训练数据具有分类能力的特征，这样可以提高决策树学习的效率。如果利用一个特征进行分类的结果与随机分类的结果没有很大的差别，则称这个特征是没有分类能力的，经验上扔掉这样的特征对决策树的学习精度影响不大。通常特征选择的准则是<strong>信息增益</strong>或<strong>信息增益比</strong>。</p>
<h2 id="熵与条件熵"><a href="#熵与条件熵" class="headerlink" title="熵与条件熵"></a>熵与条件熵</h2><p>熵(entropy)是表示随机变量不确定性的度量，设$p=[p<em>{1},p</em>{2},..,p<em>{n}]$表示一组概率分布，熵$H(p)$的定义为<br>$$<br>H(p)=-\sum</em>{i=1}^{n}p_i\log{p_i}<br>$$<br>熵越大，随机变量的不确定性也就越大。</p>
<p>当随机变量只取两个值，例如1，0时，即<br>$$<br>H(p)=-p\log{p}-(1-p)\log(1-p)<br>$$<br>熵$H(p)$随概率p变化的曲线如图所示</p>
<p><img src="/2017/11/03/决策树/2017-11-03-决策树/屏幕快照 2017-11-03 下午8.53.31.png" alt=""></p>
<p>当$p=0$或$p=1$时$H(p)=0$，随机变量完全没有不确定性。当$p=0.5$时，$H(p)=1$，熵取值最大，随机变量不确定性最大。</p>
<p>设有随机变量$(X,Y)$，其联合概率分布为<br>$$<br>P(X=x_i,Y=y<em>i)=p</em>{ij},i=1,2,..,n;j=1,2,…,m<br>$$<br>条件熵$H(Y|X)$表示在已知随机变量$X$的条件下随机变量$Y$的不确定性，定义为$X$给定条件下$Y$的条件概率分布的熵对$X$的数学期望<br>$$<br>H(Y|X)=\sum_{i=1}^{n}P(X=x_i)H(Y|X=x_i)<br>$$<br>当熵和条件熵中的概率由数据估计（特别是极大似然估计）得到时，所对应的熵与条件熵分别为<strong>经验熵</strong>和<strong>经验条件熵</strong>。</p>
<h2 id="信息增益"><a href="#信息增益" class="headerlink" title="信息增益"></a>信息增益</h2><p>信息增益表示得知特征$X$的信息而使得类$Y$的信息不确定性减少的程度。</p>
<p>特征$A$对于训练数据集$D$的信息增益$g(D,A)$，定义为集合$D$的经验熵$H(D)$与特征$A$给定条件下$D$的经验条件熵$H(D|A)$之差，即<br>$$<br>g(D,A)=H(D)-H(D|A)<br>$$<br>对于数据集$D$而言，信息增益依赖于特征，不同的特征往往具有不同的信息增益，<strong>信息增益大的特征具有更强的分类能力</strong>。</p>
<p>具体算法如下：</p>
<p>训练数据集为$D$，$|D|$表示其样本容量，即样本个数。设有$K$个类$C_k$，$k=1,2,…,K$，$|C_k|$为属于类$C_k$的样本个数。设特征$A$有$n$个不同取值${a_1,a_2,…,a_n}$，根据特征$A$的取值将$D$划分为$n$个子集$D_1,D_2,…,D_n$，$|D_i|$为$D_i$的样本个数。记子集$D_i$中属于类$C<em>i$的样本的集合为$D</em>{ik}$，$|D_{ik}|$为对应的样本个数。</p>
<p>（1）计算数据集$D$的经验熵$H(D)$<br>$$<br>H(D)=-\sum_{k=1}^{K}\frac{|C_k|}{|D|}\log\frac{|C<em>k|}{|D|}<br>$$<br>（2）计算特征$A$对数据集$D$的经验条件熵$H(D|A)$<br>$$<br>H(D|A)=\sum</em>{i=1}^{n}\frac{|D_i|}{|D|}H(D<em>i)=-\sum</em>{i=1}^{n}\frac{|D<em>i|}{|D|}\sum</em>{k=1}^{K}\frac{|D_{ik}|}{|D<em>i|}\log\frac{|D</em>{ik}|}{|D_i|}<br>$$<br>（3）计算信息增益<br>$$<br>g(D,A)=H(D)-H(D|A)<br>$$<br><strong>缺点</strong>：信息增益偏向取值较多的特征</p>
<p><strong>原因</strong>：当特征的取值较多时，根据此特征划分更容易得到纯度更高的子集，因此划分之后的熵更低，由于划分前的熵是一定的，因此信息增益更大，因此信息增益比较偏向取值较多的特征。</p>
<h2 id="信息增益比"><a href="#信息增益比" class="headerlink" title="信息增益比"></a>信息增益比</h2><p>以信息增益作为划分训练数据集的特征，存在偏向于选择取值较多的特征的问题，使用信息增益比可以对这一问题进行校正。</p>
<p>特征$A$对训练数据集$D$的信息增益比$g_R(D,A)$定义为其信息增益$g(D,A)$与训练数据集$D$关于特征$A$的值的熵$H_A(D)$之比，即<br>$$<br>g_R(D,A)=\frac{g(D,A)}{H_A(D)}  \H<em>A(D)=-\sum</em>{i=1}^n\frac{|D_i|}{|D|}\log\frac{|D_i|}{|D|}<br>$$<br><strong>信息增益比本质</strong>： 在信息增益的基础之上乘上一个惩罚参数。特征个数较多时，惩罚参数较小；特征个数较少时，惩罚参数较大。</p>
<p><strong>惩罚参数</strong>：数据集D以特征A作为随机变量的熵的倒数，即将特征A取值相同的样本划分到同一个子集中。</p>
<p><strong>缺点</strong>：信息增益比偏向取值较少的特征   </p>
<h1 id="决策树的生成"><a href="#决策树的生成" class="headerlink" title="决策树的生成"></a>决策树的生成</h1><h2 id="ID3算法"><a href="#ID3算法" class="headerlink" title="ID3算法"></a>ID3算法</h2><p>ID3算法的核心是在决策树各个结点上应用<strong>信息增益</strong>准则来选择特征，递归地构建决策树。</p>
<p>具体算法：</p>
<p>从根结点开始，对结点计算所有可能特征的信息增益，选择<strong>信息增益</strong>最大的特征$A_g$，如果$A_g$的信息增益大于阈值$\epsilon$，由该特征的不同取值建立子结点；如果$A_g$的信息增益小于阈值$\epsilon$，则所有实例都归为一个结点，实例中最大的类$C_k$为该结点的类标记。如此递归。</p>
<h2 id="C4-5算法"><a href="#C4-5算法" class="headerlink" title="C4.5算法"></a>C4.5算法</h2><p>C4.5算法的核心是在决策树各个结点上应用<strong>信息增益比</strong>准则来选择特征，递归地构建决策树。</p>
<p>具体算法：</p>
<p>从根结点开始，对结点计算所有可能特征的信息增益，选择<strong>信息增益比</strong>最大的特征$A_g$，如果$A_g$的信息增益大于阈值$\epsilon$，由该特征的不同取值建立子结点；如果$A_g$的信息增益小于阈值$\epsilon$，则所有实例都归为一个结点，实例中最大的类$C_k$为该结点的类标记。如此递归。</p>
<h1 id="决策树的剪枝"><a href="#决策树的剪枝" class="headerlink" title="决策树的剪枝"></a>决策树的剪枝</h1><p>决策树生成算法产生的树往往对训练数据的分类很准确，但对于未知的测试数据的分类没有那么准确，即出现过拟合现象。过拟合的原因在于学习时过多地考虑如何提高对训练数据的正确分类，从而构建出过于复杂的决策树。解决这一问题的办法就是<strong>剪枝</strong>。</p>
<p>决策树的剪枝通过极小化决策树整体的损失函数来实现。设数$T$的叶结点个数为$|T|$，$t$是树$T$的叶结点，该叶结点有$N<em>t$个样本点，其中$k$类的样本点有$N</em>{tk}$个，$k=1,2,…,K$，$H<em>t(T)$为叶结点$t$上的经验熵，$\alpha\geq0$为参数，损失函数定义为:<br>$$<br>C</em>{\alpha}(T)=\sum_{t=1}^{|T|}N_tH_t(T)+\alpha|T|<br>$$<br>其中，$H<em>t(T)=-\sum</em>{k}\frac{N_{tk}}{N<em>t}\log\frac{N</em>{tk}}{N_t}$</p>
<p>整理为<br>$$<br>C<em>{\alpha}(T)=-\sum</em>{t=1}^{|T|}\sum<em>{1}^{K}N</em>{tk}\log\frac{N_{tk}}{N_t}+\alpha|T|\<br>=C(T)+\alpha|T|<br>$$<br>$C(T)$表示模型对训练数据的预测误差，$|T|$表示模型复杂度。$\alpha$确定时，树越大，往往对训练数据的拟合越好，但模型的复杂度高；相反，树越小，模型的复杂度低，对训练数据的拟合不好。</p>
<p>可以看出，决策树的生成只考虑了对训练数据的拟合，而决策树的剪枝则减少模型的过拟合。</p>
<p><strong>剪枝算法</strong>具体如下</p>
<p>（1）计算每个结点的经验熵</p>
<p>（2）递归地从树的叶结点向上回缩</p>
<p>​    设一组叶结点回缩到其父结点之前与之后的整体树分别为$T_B$与$T<em>A$，其对应的损失函数值分别是$C</em>\alpha(T<em>B)$与$C</em>\alpha(T<em>A)$，如果$C</em>\alpha(T<em>A)\leq C</em>\alpha(T_B)$则进行剪枝，父结点变为新的叶结点。</p>
<p>（3）重复进行（2），直至不能继续。</p>
<h1 id="CART算法"><a href="#CART算法" class="headerlink" title="CART算法"></a>CART算法</h1><p>分类与回归树(classification and regression tree)模型是应用广泛的决策树学习方法。CART同样由特征选择、树的生成及剪枝组成，即可以用于分类也可以用于回归。以下将用于分类与回归的树统称为决策树。</p>
<p>CART是在给定输入随机变量$X$条件下输出随机变量$Y$的条件概率分布的学习方法。CART假设决策树是<strong>二叉树</strong>，内部结点特征的取值为“是”和“否”，左分支是取值为“是”的分支，右分支是取值为“否”的分支。这样的决策树等价于递归地二分每个特征，将输入空间即特征空间划分为有限个单元，并在这些单元上确定预测的概率分布，也就是输入给定的条件下输出的条件概率分布。</p>
<p><strong>CART算法由以下两步组成：</strong> </p>
<ol>
<li>决策树生成：基于训练数据集生成决策树，生成的决策树要尽量大； </li>
<li>决策树剪枝：用验证数据集最已生成的树进行剪枝并选择最优子树，这时用损失函数最小作为剪枝的标准。</li>
</ol>
<h2 id="CART生成"><a href="#CART生成" class="headerlink" title="CART生成"></a>CART生成</h2><h3 id="回归树的生成"><a href="#回归树的生成" class="headerlink" title="回归树的生成"></a>回归树的生成</h3><p>回归树用<strong>平方误差最小化</strong>准则。</p>
<p>假设$X$和$Y$对应着输入和输出变量，并且$Y$是连续变量。一个回归树对应着输入空间（特征空间）的一个划分以及在划分的输出值。假设已将输入空间划分为$M$个单元$R_1,R_2,…,R<em>M$，并且在每个单元有一个固定输出值，回归树模型可以表示为<br>$$<br>f(x)=\sum</em>{m=1}^{M}c_mI(x∈R<em>m)<br>$$<br>当输入空间的划分确定时，用平方误差$\sum</em>{x_i\in R_m}(y_i-f(x_i))^2$表示回归树的训练误差，用平方误差最小准则求解每个单元上的最优输出。易知，单元上$c_m$的最优值$\hat{c}_m$是$R_m$上所有输入实例$x_i$对应的输出$y_i$的均值，即<br>$$<br>\hat{c}_m=\text{ave}(y_i|x_i\in R_m)<br>$$<br><strong>最小二乘回归树生成算法</strong></p>
<p>在训练数据集所在的输入空间中，递归地将每个区域划分为两个子区域并决定每个子区域上的输出值，构建二叉决策树：</p>
<p>（1）选择最优切分变量$j$与切分点$s$，求解<br>$$<br>\min<em>\limits{j,s}[\min</em>\limits{c<em>1}\sum</em>{x_i\in R_1(j,s)}(y_i-c<em>1)^2+\min</em>\limits{c<em>2}\sum</em>{x_i\in R_2(j,s)}(y_i-c_2)^2]<br>$$<br>遍历变量$j$，对固定的切分变量$j$扫描切分点$s$，选择使上式达到最小值的对$(j,s)$。</p>
<p>（2）用选定的对$(j,s)$划分区域并决定相应的输出：<br>$$<br>R_1(j,s)={x|x^{(j)}\leq s},R_2(j,s)={x|x^{(j)}&gt; s}\<br>\hat{c}_1=\frac{1}{N<em>m}\sum</em>{x_i\in R_1(j,s)}y_i,\hat{c}_2=\frac{1}{N<em>m}\sum</em>{x_i\in R_2(j,s)}y_i<br>$$<br>（3）继续对两个子区域调用步骤（1）（2），直至满足停止条件。</p>
<p>（4）将输入空间划分为$M$个区域$R_1,R_2,…,R<em>M$，生成决策树：<br>$$<br>f(x)=\sum</em>{m=1}^{M}\hat{c}_mI(x\in R_M)<br>$$</p>
<h3 id="分类树的生成"><a href="#分类树的生成" class="headerlink" title="分类树的生成"></a>分类树的生成</h3><p>分类树用<strong>基尼指数</strong>选择最优特征。</p>
<p>分类问题中，假设有$K$个类，样本点属于第$k$类的概率为$p<em>k$，则概率分布的基尼指数定义为：<br>$$<br>\text{Gini}(p)=\sum</em>{k=1}^{K}p_k(1-p<em>k)=1-\sum</em>{k=1}^{K}p^2<em>k<br>$$<br>对于二元分类问题，若样本点属于第1个类的概率是$p$，则概率分布的基尼指数为<br>$$<br>\text{Gini}(p)=2p(1-p)<br>$$<br>对于给定样本集合$D$，其基尼指数为<br>$$<br>\text{Gini}(D)=1-\sum</em>{k=1}^{K}(\frac{|C_k|}{|D|})^2<br>$$<br>如果样本集合$D$根据特征$A$是否取某一可能值$a$被分割成$D_1$和 $D_2$两部分，即<br>$$<br>D_1={(X,Y)\in D|A(x)=a},D_2=D-D_1<br>$$<br>则在特征A的条件下，集合$D$的基尼指数定义为<br>$$<br>\text{Gini}({D,A})=\frac{|D_1|}{D}\text{Gini}(D_1)+\frac{|D_2|}{D}\text{Gini}(D_2)<br>$$<br>$\text{Gini}(D)$表示集合D的不确定性，基尼指数$\text{Gini}(D,A)$表示经$A=a$分割后$D$的不确定性，Gini指数越大，不确定性越大，与熵相似。</p>
<p><strong>CART树生成算法</strong></p>
<p>根据跟=训练数据集，从根结点开始，递归地对每个结点进行以下操作，构建二叉决策树：</p>
<p>（1）设结点的训练数据集为$D$，计算现有特征对数据集的基尼指数。此时，对每一个特征$A$，对其可能取的每个值$a$，根据样本点对$A=a$的测试为“是”或者“否”，将$D$分割成$D1$和$D2$两部分，计算$A=a$的基尼指数。</p>
<p>（2）在所有可能的特征$A$以及他们所有可能的切分点$a$中，选择基尼指数最小的特征及其对应的切分点作为最优特征与最优切分点。依最优特征与最优切分点，从现结点生成两个子结点，将训练数据依特征分配到两个子集中。</p>
<p>（3）对两个子结点递归地调用（1）（2），直至停止。</p>
<h2 id="CART剪枝"><a href="#CART剪枝" class="headerlink" title="CART剪枝"></a>CART剪枝</h2><p>CART剪枝算法从决策树底端剪去一些子树，使得模型变简单，从而能够对未知数据有更准确的预测。</p>
<p>算法分成两个步骤：首先从生成算法产生的决策树底端$T_0$开始不断剪枝，知道$T_0$根结点，形成一个子树序列${T_0,T_1,…,T_n}$，然后通过<strong>交叉验证</strong>法在独立的验证数据集上对子树进行测试，从中选择最优子树。</p>
<ol>
<li><p>剪枝，剪枝形成一个子树序列</p>
<p>子树的损失函数：</p>
</li>
</ol>
<p>$$<br>C_{\alpha}(T)=C(T)+\alpha|T|<br>$$</p>
<p>​    对固定的$\alpha$，一定存在使损失函数$C<em>{\alpha}(T)$最小的子树，将其表示为$T</em>{\alpha}$。</p>
<p>​    具体地，从整体树$T_0$开始剪枝。对$T<em>0$的任意内部结点$t$，以$t$为单结点树的损失函数是<br>$$<br>C</em>{\alpha}(t)=C(t)+\alpha<br>$$<br>​    以$t$为根的子树$T<em>t$损失函数是<br>$$<br>C</em>{\alpha}(T_t)=C(T_t)+\alpha|T<em>t|<br>$$<br>​    当$\alpha=0$及$\alpha$充分小时，有不等式<br>$$<br>C</em>{\alpha}(T<em>t)&lt;C</em>{\alpha}(t)<br>$$<br>​    当$\alpha$增大时，在某一$\alpha$有<br>$$<br>C_{\alpha}(T<em>t)=C</em>{\alpha}(t)<br>$$<br>​    只要$\alpha=\frac{C(t)-C(T_t)}{|T_t|-1}$，$T_t$与$t$有相同的损失函数值，而$t$的结点少，因此$t$更可取，对$T_t$进行剪枝。</p>
<p>​    为此，对$T_0$中每一内部结点$t$，计算<br>$$<br>g(t)=\frac{C(t)-C(T_t)}{|T_t|-1}<br>$$<br>​    它表示剪枝后整体损失函数减少的程度。在$T_0$中剪去$g(t)$最小的$T_t$，将得到的子树作为$T_1$，同时将最小的$g(t)$设为$\alpha_1$。</p>
<p>​    如此剪枝下去，直至得到根结点。</p>
<ol>
<li><p>在剪枝得到的子树序列$T_0,T_1,…T<em>n$中通过交叉验证选取最优子树$T</em>{\alpha}$</p>
<p>具体地，利用独立的验证数据集，测试子树序列$T_0,T_1,…,T_n$中各棵子树的平方误差或基尼指数。平方误差或基尼指数最小的决策树被认为是最优的决策树。当最优子树$T_k$确定时，对应的$\alpha$也确定了。</p>
</li>
</ol>
<h2 id="ID3、C4-5-和-CART的区别"><a href="#ID3、C4-5-和-CART的区别" class="headerlink" title="ID3、C4.5 和 CART的区别"></a>ID3、C4.5 和 CART的区别</h2><ul>
<li><p>理论上</p>
<p>ID3采用<strong>信息增益</strong>度量，C4.5采用<strong>信息增益比</strong>度量，CART采用<strong>基尼指数</strong>度量。</p>
</li>
<li><p>应用上</p>
<p>ID3和C4.5只可以用于分类， CART可用于回归和分类。</p>
</li>
<li><p>结构上</p>
<p>ID3和C4.5的树可以有很多分支，CART只用二叉树。</p>
</li>
</ul>
<ul>
<li><p>样本特征上的差异：</p>
<p>ID3和C4.5中特征只单次使用，CART可多次重复使用。</p>
</li>
</ul>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/决策树/" rel="tag"># 决策树</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2017/10/26/git的基本命令/" rel="next" title="git的基本命令">
                <i class="fa fa-chevron-left"></i> git的基本命令
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2017/11/08/集成学习(Ensemble Learning)/" rel="prev" title="集成学习法(Ensemble Learning)">
                集成学习法(Ensemble Learning) <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          
  <div class="comments" id="comments">
    
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/images/avatar.jpg"
               alt="Zejian" />
          <p class="site-author-name" itemprop="name">Zejian</p>
           
              <p class="site-description motion-element" itemprop="description"></p>
          
        </div>
        <nav class="site-state motion-element">

          
            <div class="site-state-item site-state-posts">
              <a href="/archives/">
                <span class="site-state-item-count">12</span>
                <span class="site-state-item-name">日志</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-categories">
              <a href="/categories/index.html">
                <span class="site-state-item-count">3</span>
                <span class="site-state-item-name">分类</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-tags">
              <a href="/tags/index.html">
                <span class="site-state-item-count">35</span>
                <span class="site-state-item-name">标签</span>
              </a>
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
                <a href="https://github.com/lizejian" target="_blank" title="GitHub">
                  
                    <i class="fa fa-fw fa-github"></i>
                  
                    
                      GitHub
                    
                </a>
              </span>
            
          
        </div>

        
        

        
        

        


      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#决策树模型与学习"><span class="nav-number">1.</span> <span class="nav-text">决策树模型与学习</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#决策树模型"><span class="nav-number">1.1.</span> <span class="nav-text">决策树模型</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#优点与缺点"><span class="nav-number">1.2.</span> <span class="nav-text">优点与缺点</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#决策树学习"><span class="nav-number">1.3.</span> <span class="nav-text">决策树学习</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#特征选择"><span class="nav-number">2.</span> <span class="nav-text">特征选择</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#熵与条件熵"><span class="nav-number">2.1.</span> <span class="nav-text">熵与条件熵</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#信息增益"><span class="nav-number">2.2.</span> <span class="nav-text">信息增益</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#信息增益比"><span class="nav-number">2.3.</span> <span class="nav-text">信息增益比</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#决策树的生成"><span class="nav-number">3.</span> <span class="nav-text">决策树的生成</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#ID3算法"><span class="nav-number">3.1.</span> <span class="nav-text">ID3算法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#C4-5算法"><span class="nav-number">3.2.</span> <span class="nav-text">C4.5算法</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#决策树的剪枝"><span class="nav-number">4.</span> <span class="nav-text">决策树的剪枝</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#CART算法"><span class="nav-number">5.</span> <span class="nav-text">CART算法</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#CART生成"><span class="nav-number">5.1.</span> <span class="nav-text">CART生成</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#回归树的生成"><span class="nav-number">5.1.1.</span> <span class="nav-text">回归树的生成</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#分类树的生成"><span class="nav-number">5.1.2.</span> <span class="nav-text">分类树的生成</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#CART剪枝"><span class="nav-number">5.2.</span> <span class="nav-text">CART剪枝</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#ID3、C4-5-和-CART的区别"><span class="nav-number">5.3.</span> <span class="nav-text">ID3、C4.5 和 CART的区别</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2017</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Zejian</span>
</div>


<div class="powered-by">
  由 <a class="theme-link" href="https://hexo.io">Hexo</a> 强力驱动
</div>

<div class="theme-info">
  主题 -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Mist
  </a>
</div>


        
<div class="busuanzi-count">
  <script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="site-uv">
      <i class="fa fa-user"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
      人
    </span>
  

  
</div>


        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.2"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.2"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.2"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.2"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.2"></script>



  


  




	





  





  






  





  

  

  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->
  


  

  

</body>
</html>
